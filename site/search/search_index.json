{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GraphQL for Spring Boot The DGS framework makes implementing a GraphQL server with Spring Boot easy. The framework includes features such as: First class support for both Java and Kotlin Annotation based Spring Boot programming model Test framework for writing query tests as unit tests Gradle Code Generation plugin to create types from schema Easy integration with GraphQL Federation Integration with Spring Security GraphQL subscriptions (WebSockets and SSE) File uploads Error handling Many extension points The framework started in 2019 when Netflix started developing many GraphQL services. At the end of 2020 Netflix decided to open source the framework and build a community around it. Getting started Jump right in with the tutorial ! Q & A Why start with release version 3.x? Netflix developed and used the framework over the course of almost two years before open sourcing, which involved many releases. After open sourcing the project, we are now using the OSS project internally as well. We did have to wipe out the git history, but continued the versioning we were already using. Is it production ready? Yes! Netflix has been using the framework for over a year and a half in different parts of our organisation, including at large scale, before it was open sourced. We've had many releases adding new features, fixing bugs etc., and it has become a very stable platform. Why not just use graphql-java? The DGS framework is built on top of graphql-java . Graphql-java is, and should be, lower level building blocks to handle query execution and such. The DGS framework makes all this available with a convenient Spring Boot programming model. The framework has a lot of Kotlin code, can I use it with Java? The DGS framework is primarily designed to be used with Java. Although it's primarily written in Kotlin, most consumers of the framework are Java. Of course, if you are using Kotlin, that works great too. Does Netflix run on a fork of the framework? No, Netflix is using the same OSS components! We do have some extra modules plugged in for distributed tracing, logging, metrics etc, and we have documentation that shows how to implement similar integrations for your own infrastructure.","title":"Home"},{"location":"#graphql-for-spring-boot","text":"The DGS framework makes implementing a GraphQL server with Spring Boot easy. The framework includes features such as: First class support for both Java and Kotlin Annotation based Spring Boot programming model Test framework for writing query tests as unit tests Gradle Code Generation plugin to create types from schema Easy integration with GraphQL Federation Integration with Spring Security GraphQL subscriptions (WebSockets and SSE) File uploads Error handling Many extension points The framework started in 2019 when Netflix started developing many GraphQL services. At the end of 2020 Netflix decided to open source the framework and build a community around it.","title":"GraphQL for Spring Boot"},{"location":"#getting-started","text":"Jump right in with the tutorial !","title":"Getting started"},{"location":"#q-a","text":"","title":"Q &amp; A"},{"location":"#why-start-with-release-version-3x","text":"Netflix developed and used the framework over the course of almost two years before open sourcing, which involved many releases. After open sourcing the project, we are now using the OSS project internally as well. We did have to wipe out the git history, but continued the versioning we were already using.","title":"Why start with release version 3.x?"},{"location":"#is-it-production-ready","text":"Yes! Netflix has been using the framework for over a year and a half in different parts of our organisation, including at large scale, before it was open sourced. We've had many releases adding new features, fixing bugs etc., and it has become a very stable platform.","title":"Is it production ready?"},{"location":"#why-not-just-use-graphql-java","text":"The DGS framework is built on top of graphql-java . Graphql-java is, and should be, lower level building blocks to handle query execution and such. The DGS framework makes all this available with a convenient Spring Boot programming model.","title":"Why not just use graphql-java?"},{"location":"#the-framework-has-a-lot-of-kotlin-code-can-i-use-it-with-java","text":"The DGS framework is primarily designed to be used with Java. Although it's primarily written in Kotlin, most consumers of the framework are Java. Of course, if you are using Kotlin, that works great too.","title":"The framework has a lot of Kotlin code, can I use it with Java?"},{"location":"#does-netflix-run-on-a-fork-of-the-framework","text":"No, Netflix is using the same OSS components! We do have some extra modules plugged in for distributed tracing, logging, metrics etc, and we have documentation that shows how to implement similar integrations for your own infrastructure.","title":"Does Netflix run on a fork of the framework?"},{"location":"data-loaders/","text":"Data loaders solve the N+1 problem while loading data. The N+1 Problem Explained Say you query for a list of movies, and each movie includes some data about the director of the movie. Also assume that the Movie and Director [entities] are owned by two different services. In a na\u00efve implementation, to load 50 movies, you would have to call the Director service 50 times: once for each movie. This totals 51 queries: one query to get the list of movies, and 50 queries to get the director data for each movie. This obviously wouldn\u2019t perform very well. It would be much more efficient to create a list of directors to load, and load all of them at once in a single call. This first of all must be supported by the Director service , because that service needs to provide a way to load a list of Directors. The data fetchers in the Movie service need to be smart as well, to take care of batching the requests to the Directors service. This is where data loaders come in. What If My Service Doesn\u2019t Support Loading in Batches? What if (in this example) DirectorServiceClient doesn\u2019t provide a method to load a list of directors? What if it only provides a method to load a single director by ID? The same problem applies to REST services as well: what if there\u2019s no endpoint to load multiple directors? Similarly, to load from a database directly, you must write a query to load multiple directors. If such methods are unavailable, the providing service needs to fix this! Implementing a Data Loader The easiest way for you to register a data loader is for you to create a class that implements the org.dataloader.BatchLoader or org.dataloader.MappedBatchLoader interface. This interface is parameterized; it requires a type for the key and result of the BatchLoader . For example, if the identifiers for a Director are of type String , you could have a org.dataloader.BatchLoader<String, Director> . You must annotate the class with @DgsDataLoader so that the framework will register the data loader it represents. In order to implement the BatchLoader interface you must implement a CompletionStage<List<V>> load(List<K> keys) method. The following example is a data loader that loads data from an imaginary Director service: package com.netflix.graphql.dgs.example.dataLoader; import com.netflix.graphql.dgs.DgsDataLoader; import org.dataloader.BatchLoader; import java.util.List; import java.util.concurrent.CompletableFuture; import java.util.concurrent.CompletionStage; import java.util.stream.Collectors; @DgsDataLoader(name = \"directors\") public class DirectorsDataLoader implements BatchLoader<String, Director> { @Autowired DirectorServiceClient directorServiceClient; @Override public CompletionStage<List<Director>> load(List<String> keys) { return CompletableFuture.supplyAsync(() -> directorServiceClient.loadDirectors(keys)); } } The data loader is responsible for loading data for a given list of keys. In this example, it just passes on the list of keys to the backend that owns Director (this could for example be a [gRPC] service). However, you might also write such a service so that it loads data from a database. Although this example registers a data loader, nobody will use that data loader until you implement a data fetcher that uses it. Provide as Lambda Because BatchLoader is a functional interface (an interface with only a single method), you can also provide it as a lambda expression. Technically this is exactly the same as providing a class; it\u2019s really just another way of writing it: @DgsComponent public class ExampleBatchLoaderFromField { @DgsDataLoader(name = \"directors\") public BatchLoader<String, Director> directorBatchLoader = keys -> CompletableFuture.supplyAsync(() -> directorServiceClient.loadDirectors(keys)); } MappedBatchLoader The BatchLoader interface creates a List of values for a List of keys. You can also use the MappedBatchLoader which creates a Map of key/values for a Set of values. The latter is a better choice if you do not expect all keys to have a value. You register a MappedBatchLoader in the same way as you register a BatchLoader : @DgsDataLoader(name = \"directors\") public class DirectorsDataLoader implements MappedBatchLoader<String, Director> { @Autowired DirectorServiceClient directorServiceClient; @Override public CompletionStage<Map<String, Director>> load(Set<String> keys) { return CompletableFuture.supplyAsync(() -> directorServiceClient.loadDirectors(keys)); } } Using a Data Loader The following is an example of a data fetcher that uses a data loader: @DgsComponent public class DirectorDataFetcher { @DgsData(parentType = \"Movie\", field = \"director\") public CompletableFuture<Director> director(DataFetchingEnvironment dfe) { DataLoader<String, Director> dataLoader = dfe.getDataLoader(\"directors\"); String id = dfe.getArgument(\"directorId\"); return dataLoader.load(id); } } The code above is mostly just a regular data fetcher. However, instead of actually loading the data from another service or database, it uses the data loader to do so. You can retrieve a data loader from the DataFetchingEnvironment with its getDataLoader() method. This requires you to pass the name of the data loader as a string. The other change to the data fetcher is that it returns a CompletableFuture instead of the actual type you\u2019re loading. This enables the framework to do work asynchronously, and is a requirement for batching . Using the DgsDataFetchingEnvironment You can also get the data loader in a type-safe way by using our custom DgsDataFetchingEnvironment , which is an enhanced version of DataFetchingEnvironment in graphql-java , and provides getDataLoader() using the classname. @DgsComponent public class DirectorDataFetcher { @DgsData(parentType = \"Movie\", field = \"director\") public CompletableFuture<Director> director(DgsDataFetchingEnvironment dfe) { DataLoader<String, Director> dataLoader = dfe.getDataLoader(DirectorsDataLoader.class); String id = dfe.getArgument(\"directorId\"); return dataLoader.load(id); } } !!!tip The same works if you have @DgsDataLoader defined as a lambda instead of on a class as shown here . If you have multiple @DgsDataLoader lambdas defined as fields in the same class, you won't be able to use this feature. It is recommended that you use getDataLoader() with the loader name passed as a string in such cases. Note that there is no logic present about how batching works exactly; this is all handled by the framework! The framework will recognize that many directors need to be loaded when many movies are loaded, batch up all the calls to the data loader, and call the data loader with a list of IDs instead of a single ID. The data loader implemented above already knows how to handle a list of IDs, and that way it avoids the N+1 problem. Using Spring Features such as SsoCallerResolver inside a CompletableFuture When you write async data fetchers, the code will run on worker threads. Spring internally stores some context, for example to make the SsoCallerResolver work, on the thread context however. This context wouldn\u2019t be available inside code running on a different thread, which makes features such as SsoCallerResolver not work. Spring Boot has a solution for this: it manages a thread pool that does have this context carry over. You can inject this solution in the following way: @Autowired @DefaultExecutor private Executor executor; You must pass in the executor as the second argument of the supplyAsync() method which is typically used to make data fetchers asynchronous. @DgsData(parentType = \"Query\", field = \"list_things\") public CompletableFuture<List<Thing>> resolve(DataFetchingEnvironment environment) { return CompletableFuture.supplyAsync(() -> { return myService.getThings(); }, executor); Caching Batching is the most important aspect of preventing N+1 problems. Data loaders also support caching, however. If the same key is loaded multiple times, it will only be loaded once. For example, if a list of movies is loaded , and some movies are directed by the same director, the director data will only be retrieved once. !!!info \"Caching is Disabled by Default in DGS 1\" Version 1 of the DGS framework disables caching by default, but you can switch it on in the @DgsDataLoader annotation: @DgsDataLoader(name = \"directors\", caching=true) class DirectorsBatchLoader implements BatchLoader<String, Director> {} You do not need to make this change in version 2 of the DGS framework, because that version enables caching by default. Batch Size Sometimes it\u2019s possible to load multiple items at once, but to a certain limit. When loading from a database for example, an IN query could be used , but maybe with the limitation of a maximum number of IDs to provide. The @DgsDataLoader has a maxBatchSize annotation that you can use to configure this behavior. By default it does not specify a maximum batch size. Data Loader Scope Data loaders are wired up to only span a single request. This is what most use cases require. Spanning multiple requests can introduce difficult-to-debug issues. --8<-- \"docs/reference_links\"","title":"Async Data Fetching"},{"location":"data-loaders/#the-n1-problem-explained","text":"Say you query for a list of movies, and each movie includes some data about the director of the movie. Also assume that the Movie and Director [entities] are owned by two different services. In a na\u00efve implementation, to load 50 movies, you would have to call the Director service 50 times: once for each movie. This totals 51 queries: one query to get the list of movies, and 50 queries to get the director data for each movie. This obviously wouldn\u2019t perform very well. It would be much more efficient to create a list of directors to load, and load all of them at once in a single call. This first of all must be supported by the Director service , because that service needs to provide a way to load a list of Directors. The data fetchers in the Movie service need to be smart as well, to take care of batching the requests to the Directors service. This is where data loaders come in.","title":"The N+1 Problem Explained"},{"location":"data-loaders/#what-if-my-service-doesnt-support-loading-in-batches","text":"What if (in this example) DirectorServiceClient doesn\u2019t provide a method to load a list of directors? What if it only provides a method to load a single director by ID? The same problem applies to REST services as well: what if there\u2019s no endpoint to load multiple directors? Similarly, to load from a database directly, you must write a query to load multiple directors. If such methods are unavailable, the providing service needs to fix this!","title":"What If My Service Doesn\u2019t Support Loading in Batches?"},{"location":"data-loaders/#implementing-a-data-loader","text":"The easiest way for you to register a data loader is for you to create a class that implements the org.dataloader.BatchLoader or org.dataloader.MappedBatchLoader interface. This interface is parameterized; it requires a type for the key and result of the BatchLoader . For example, if the identifiers for a Director are of type String , you could have a org.dataloader.BatchLoader<String, Director> . You must annotate the class with @DgsDataLoader so that the framework will register the data loader it represents. In order to implement the BatchLoader interface you must implement a CompletionStage<List<V>> load(List<K> keys) method. The following example is a data loader that loads data from an imaginary Director service: package com.netflix.graphql.dgs.example.dataLoader; import com.netflix.graphql.dgs.DgsDataLoader; import org.dataloader.BatchLoader; import java.util.List; import java.util.concurrent.CompletableFuture; import java.util.concurrent.CompletionStage; import java.util.stream.Collectors; @DgsDataLoader(name = \"directors\") public class DirectorsDataLoader implements BatchLoader<String, Director> { @Autowired DirectorServiceClient directorServiceClient; @Override public CompletionStage<List<Director>> load(List<String> keys) { return CompletableFuture.supplyAsync(() -> directorServiceClient.loadDirectors(keys)); } } The data loader is responsible for loading data for a given list of keys. In this example, it just passes on the list of keys to the backend that owns Director (this could for example be a [gRPC] service). However, you might also write such a service so that it loads data from a database. Although this example registers a data loader, nobody will use that data loader until you implement a data fetcher that uses it.","title":"Implementing a Data Loader"},{"location":"data-loaders/#provide-as-lambda","text":"Because BatchLoader is a functional interface (an interface with only a single method), you can also provide it as a lambda expression. Technically this is exactly the same as providing a class; it\u2019s really just another way of writing it: @DgsComponent public class ExampleBatchLoaderFromField { @DgsDataLoader(name = \"directors\") public BatchLoader<String, Director> directorBatchLoader = keys -> CompletableFuture.supplyAsync(() -> directorServiceClient.loadDirectors(keys)); }","title":"Provide as Lambda"},{"location":"data-loaders/#mappedbatchloader","text":"The BatchLoader interface creates a List of values for a List of keys. You can also use the MappedBatchLoader which creates a Map of key/values for a Set of values. The latter is a better choice if you do not expect all keys to have a value. You register a MappedBatchLoader in the same way as you register a BatchLoader : @DgsDataLoader(name = \"directors\") public class DirectorsDataLoader implements MappedBatchLoader<String, Director> { @Autowired DirectorServiceClient directorServiceClient; @Override public CompletionStage<Map<String, Director>> load(Set<String> keys) { return CompletableFuture.supplyAsync(() -> directorServiceClient.loadDirectors(keys)); } }","title":"MappedBatchLoader"},{"location":"data-loaders/#using-a-data-loader","text":"The following is an example of a data fetcher that uses a data loader: @DgsComponent public class DirectorDataFetcher { @DgsData(parentType = \"Movie\", field = \"director\") public CompletableFuture<Director> director(DataFetchingEnvironment dfe) { DataLoader<String, Director> dataLoader = dfe.getDataLoader(\"directors\"); String id = dfe.getArgument(\"directorId\"); return dataLoader.load(id); } } The code above is mostly just a regular data fetcher. However, instead of actually loading the data from another service or database, it uses the data loader to do so. You can retrieve a data loader from the DataFetchingEnvironment with its getDataLoader() method. This requires you to pass the name of the data loader as a string. The other change to the data fetcher is that it returns a CompletableFuture instead of the actual type you\u2019re loading. This enables the framework to do work asynchronously, and is a requirement for batching .","title":"Using a Data Loader"},{"location":"data-loaders/#using-the-dgsdatafetchingenvironment","text":"You can also get the data loader in a type-safe way by using our custom DgsDataFetchingEnvironment , which is an enhanced version of DataFetchingEnvironment in graphql-java , and provides getDataLoader() using the classname. @DgsComponent public class DirectorDataFetcher { @DgsData(parentType = \"Movie\", field = \"director\") public CompletableFuture<Director> director(DgsDataFetchingEnvironment dfe) { DataLoader<String, Director> dataLoader = dfe.getDataLoader(DirectorsDataLoader.class); String id = dfe.getArgument(\"directorId\"); return dataLoader.load(id); } } !!!tip The same works if you have @DgsDataLoader defined as a lambda instead of on a class as shown here . If you have multiple @DgsDataLoader lambdas defined as fields in the same class, you won't be able to use this feature. It is recommended that you use getDataLoader() with the loader name passed as a string in such cases. Note that there is no logic present about how batching works exactly; this is all handled by the framework! The framework will recognize that many directors need to be loaded when many movies are loaded, batch up all the calls to the data loader, and call the data loader with a list of IDs instead of a single ID. The data loader implemented above already knows how to handle a list of IDs, and that way it avoids the N+1 problem.","title":"Using the DgsDataFetchingEnvironment"},{"location":"data-loaders/#using-spring-features-such-as-ssocallerresolver-inside-a-completablefuture","text":"When you write async data fetchers, the code will run on worker threads. Spring internally stores some context, for example to make the SsoCallerResolver work, on the thread context however. This context wouldn\u2019t be available inside code running on a different thread, which makes features such as SsoCallerResolver not work. Spring Boot has a solution for this: it manages a thread pool that does have this context carry over. You can inject this solution in the following way: @Autowired @DefaultExecutor private Executor executor; You must pass in the executor as the second argument of the supplyAsync() method which is typically used to make data fetchers asynchronous. @DgsData(parentType = \"Query\", field = \"list_things\") public CompletableFuture<List<Thing>> resolve(DataFetchingEnvironment environment) { return CompletableFuture.supplyAsync(() -> { return myService.getThings(); }, executor);","title":"Using Spring Features such as SsoCallerResolver inside a CompletableFuture"},{"location":"data-loaders/#caching","text":"Batching is the most important aspect of preventing N+1 problems. Data loaders also support caching, however. If the same key is loaded multiple times, it will only be loaded once. For example, if a list of movies is loaded , and some movies are directed by the same director, the director data will only be retrieved once. !!!info \"Caching is Disabled by Default in DGS 1\" Version 1 of the DGS framework disables caching by default, but you can switch it on in the @DgsDataLoader annotation: @DgsDataLoader(name = \"directors\", caching=true) class DirectorsBatchLoader implements BatchLoader<String, Director> {} You do not need to make this change in version 2 of the DGS framework, because that version enables caching by default.","title":"Caching"},{"location":"data-loaders/#batch-size","text":"Sometimes it\u2019s possible to load multiple items at once, but to a certain limit. When loading from a database for example, an IN query could be used , but maybe with the limitation of a maximum number of IDs to provide. The @DgsDataLoader has a maxBatchSize annotation that you can use to configure this behavior. By default it does not specify a maximum batch size.","title":"Batch Size"},{"location":"data-loaders/#data-loader-scope","text":"Data loaders are wired up to only span a single request. This is what most use cases require. Spanning multiple requests can introduce difficult-to-debug issues. --8<-- \"docs/reference_links\"","title":"Data Loader Scope"},{"location":"federated-testing/","text":"Federation allows you to extend or reference existing types in a graph. Your DGS fulfills a part of the query based on the schema that is owned by your DGS, while the gateway is responsible for fetching data from other DGSs. !!!tip \"There is more federation documentation available\" * Look at the Federation example app , including testing. A tutorial style walkthrough of this example is available here . Testing Federated Queries without the Gateway You can test federated queries for your DGS in isolation by replicating the format of the query that the gateway would send to your DGS. This does not involve the gateway, and thus the parts of the query response that your DGS is not responsible for will not be hydrated. This technique is useful if you want to verify that your DGS is able to return the appropriate data, in response to a federated query. Let's look at an example of a schema that extends the Movie type that is already defined by another DGS. type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } Now you want to verify that your DGS is able to fulfill the Movie query by hydrating the script field based on the movieId field. Normally, the gateway would send an _entities query in the following format: query ($representations: [_Any!]!) { _entities(representations: $representations) { ... on Movie { movieId script { title } }}} The representations input is a variable map containing the __typename field set to Movie and movieId set to a value, e.g., 12345 . You can now set up a Query Executor test by either manually constructing the query, or you can generate the federated query using the Entities Query Builder API available through client code generation . Here is an example of a test that uses a manually constructed _entities query for Movie : @Test void federatedMovieQuery() throws IOException { String query = \"query ($representations: [_Any!]!) {\" + \"_entities(representations: $representations) {\" + \"... on Movie {\" + \"movieId \" + \"script { title }\" + \"}}}\"; Map<String, Object> variables = new HashMap<>(); Map<String,Object> representation = new HashMap<>(); representation.put(\"__typename\", \"Movie\"); representation.put(\"movieId\", 1); variables.put(\"representations\", List.of(representation)); DocumentContext context = queryExecutor.executeAndGetDocumentContext(query, variables); GraphQLResponse response = new GraphQLResponse(context.jsonString()); Movie movie = response.extractValueAsObject(\"data._entities[0]\", Movie.class); assertThat(movie.getScript().getTitle()).isEqualTo(\"Top Secret\"); } Using the Entities Query Builder API Alternatively, you can generate the federated query by using EntitiesGraphQLQuery to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . You will also need to add com.netflix.graphql.dgs:graphql-dgs-client:latest.release dependency to build.gradle. Now we can write a test that uses EntitiesGraphQLQuery along with GraphQLQueryRequest and EntitiesProjectionRoot to build the query. Finally, you can also extract the response using GraphQLResponse . This set up is shown here: @Test void federatedMovieQueryAPI() throws IOException { // constructs the _entities query with variable $representations containing a // movie representation that represents { __typename: \"Movie\" movieId: 12345 } EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery.Builder() .addRepresentationAsVariable( MovieRepresentation.newBuilder().movieId(1122).build() ) .build(); // sets up the query and the field selection set using the EntitiesProjectionRoot GraphQLQueryRequest request = new GraphQLQueryRequest( entitiesQuery, new EntitiesProjectionRoot().onMovie().movieId().script().title()); String query = request.serialize(); // pass in the constructed _entities query with the variable map containing representations DocumentContext context = queryExecutor.executeAndGetDocumentContext(query, entitiesQuery.getVariables()); GraphQLResponse response = new GraphQLResponse(context.jsonString()); Movie movie = response.extractValueAsObject(\"data._entities[0]\", Movie.class); assertThat(movie.getScript().getTitle()).isEqualTo(\"Top Secret\"); } Check out this video for a demo on how to configure and write the above test. For a complete example of federation, please check out these docs . Testing Federated Queries with the Gateway and Other DGSs For a complete federated query test, you will need the gateway to talk to other DGSs as well for hydrating all the data for the response. You can do this by configuring your local gateway to talk to your DGS on the local host, while communicating with other deployed DGSs in the test environment. The gateway fetches your local DGS schema via introspection, and the remainder of the graph from DGSs deployed in test. Now you can run the same query against the gateway and verify the entire response. The setup is described here . --8<-- \"docs/reference_links\"","title":"Federated testing"},{"location":"federated-testing/#testing-federated-queries-without-the-gateway","text":"You can test federated queries for your DGS in isolation by replicating the format of the query that the gateway would send to your DGS. This does not involve the gateway, and thus the parts of the query response that your DGS is not responsible for will not be hydrated. This technique is useful if you want to verify that your DGS is able to return the appropriate data, in response to a federated query. Let's look at an example of a schema that extends the Movie type that is already defined by another DGS. type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } Now you want to verify that your DGS is able to fulfill the Movie query by hydrating the script field based on the movieId field. Normally, the gateway would send an _entities query in the following format: query ($representations: [_Any!]!) { _entities(representations: $representations) { ... on Movie { movieId script { title } }}} The representations input is a variable map containing the __typename field set to Movie and movieId set to a value, e.g., 12345 . You can now set up a Query Executor test by either manually constructing the query, or you can generate the federated query using the Entities Query Builder API available through client code generation . Here is an example of a test that uses a manually constructed _entities query for Movie : @Test void federatedMovieQuery() throws IOException { String query = \"query ($representations: [_Any!]!) {\" + \"_entities(representations: $representations) {\" + \"... on Movie {\" + \"movieId \" + \"script { title }\" + \"}}}\"; Map<String, Object> variables = new HashMap<>(); Map<String,Object> representation = new HashMap<>(); representation.put(\"__typename\", \"Movie\"); representation.put(\"movieId\", 1); variables.put(\"representations\", List.of(representation)); DocumentContext context = queryExecutor.executeAndGetDocumentContext(query, variables); GraphQLResponse response = new GraphQLResponse(context.jsonString()); Movie movie = response.extractValueAsObject(\"data._entities[0]\", Movie.class); assertThat(movie.getScript().getTitle()).isEqualTo(\"Top Secret\"); }","title":"Testing Federated Queries without the Gateway"},{"location":"federated-testing/#using-the-entities-query-builder-api","text":"Alternatively, you can generate the federated query by using EntitiesGraphQLQuery to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . You will also need to add com.netflix.graphql.dgs:graphql-dgs-client:latest.release dependency to build.gradle. Now we can write a test that uses EntitiesGraphQLQuery along with GraphQLQueryRequest and EntitiesProjectionRoot to build the query. Finally, you can also extract the response using GraphQLResponse . This set up is shown here: @Test void federatedMovieQueryAPI() throws IOException { // constructs the _entities query with variable $representations containing a // movie representation that represents { __typename: \"Movie\" movieId: 12345 } EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery.Builder() .addRepresentationAsVariable( MovieRepresentation.newBuilder().movieId(1122).build() ) .build(); // sets up the query and the field selection set using the EntitiesProjectionRoot GraphQLQueryRequest request = new GraphQLQueryRequest( entitiesQuery, new EntitiesProjectionRoot().onMovie().movieId().script().title()); String query = request.serialize(); // pass in the constructed _entities query with the variable map containing representations DocumentContext context = queryExecutor.executeAndGetDocumentContext(query, entitiesQuery.getVariables()); GraphQLResponse response = new GraphQLResponse(context.jsonString()); Movie movie = response.extractValueAsObject(\"data._entities[0]\", Movie.class); assertThat(movie.getScript().getTitle()).isEqualTo(\"Top Secret\"); } Check out this video for a demo on how to configure and write the above test. For a complete example of federation, please check out these docs .","title":"Using the Entities Query Builder API"},{"location":"federated-testing/#testing-federated-queries-with-the-gateway-and-other-dgss","text":"For a complete federated query test, you will need the gateway to talk to other DGSs as well for hydrating all the data for the response. You can do this by configuring your local gateway to talk to your DGS on the local host, while communicating with other deployed DGSs in the test environment. The gateway fetches your local DGS schema via introspection, and the remainder of the graph from DGSs deployed in test. Now you can run the same query against the gateway and verify the entire response. The setup is described here . --8<-- \"docs/reference_links\"","title":"Testing Federated Queries with the Gateway and Other DGSs"},{"location":"generating-code-from-schema/","text":"The [DGS] Code Generation plugin generates code during your project\u2019s build process based on your Domain Graph Service\u2019s [GraphQL] schema file. The plugin generates the following: Data types for types, input types, enums and interfaces. A DgsConstants class containing the names of types and fields Example data fetchers A type safe query API that represents your queries Quick Start To apply the plugin, update your project\u2019s build.gradle file to include the following: buildscript { dependencies{ classpath 'netflix:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'codegen-gradle-plugin' generateJava{ schemaPaths = [\"${projectDir}/src/main/resources/schema\"] // List of directories containing schema files packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true // Enable generating the type safe query API } The plugin adds a generateJava [Gradle] task that runs as part of your project\u2019s build. generateJava generates the code in the project\u2019s build/generated directory. This folder is automatically added to the project's classpath. Types are available as part of the package specified by the packageName .types , where you specify the value of packageName as a configuration in your build.gradle file. Please ensure that your project\u2019s sources refer to the generated code using the specified package name. generateJava generates the data fetchers and places them in build/generated-examples . !!!note generateJava does not add the data fetchers that it generates to your project\u2019s sources. These fetchers serve mainly as a basic boilerplate code that require further implementation from you. You can exclude parts of the schema from code-generation by placing them in a different schema directory that is not specified as part of the schemaPaths for the plugin. Mapping existing types Codegen tries to generate a type for each type it finds in the schema, with a few exceptions. Basic scalar types - are mapped to corresponding Java/Kotlin types (String, Integer etc.) Data and time types - are mapped to corresponding java.time classes PageInfo and RelayPageInfo - are mapped to graphql.relay classes Types mapped with a typeMapping configuration When you have existing classes that you want to use instead of generating a class for a certain type, you can configure the plugin to do so using a typeMapping . The typeMapping configuration is a Map where each key is a GraphQL type and each value is a fully qualified Java/Kotlin type. generateJava{ typeMapping = [\"MyGraphQLType\": \"com.mypackage.MyJavaType\"] } Generating Client APIs The code generator can also create client API classes. You can use these classes to query data from a GraphQL endpoint using Java. Java GraphQL clients are useful for server-to-server communication and testing. !!!note Remember that although server-to-server GraphQL is possible, gRPC is a better choice in most cases. !!!note A DGS behind the Studio Edge Gateway should never have to call another DGS behind the gateway directly. Use federation instead! Code generation creates a field-name GraphQLQuery for each Query and Mutation field. The *GraphQLQuery query class contains fields for each parameter of the field. For each type returned by a Query or Mutation, code generation creates a *Projection . A projection is a builder class that specifies which fields get returned. The following is an example usage of a generated API: GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new TicksGraphQLQuery.Builder() .first(first) .after(after) .build(), new TicksConnectionProjection() .edges() .node() .date() .route() .name() .votes() .starRating() .parent() .grade()); This API was generated based on the following schema. The edges and node types are because the schema uses pagination. The API allows for a fluent style of writing queries, with almost the same feel of writing the query as a String, but with the added benefit of code completion and type safety. type Query @extends { ticks(first: Int, after: Int, allowCached: Boolean): TicksConnection } type Tick { id: ID route: Route date: LocalDate userStars: Int userRating: String leadStyle: LeadStyle comments: String } type Votes { starRating: Float nrOfVotes: Int } type Route { routeId: ID name: String grade: String style: Style pitches: Int votes: Votes location: [String] } type TicksConnection { edges: [TickEdge] } type TickEdge { cursor: String! node: Tick } Sending a Query A GraphQLQueryRequest can be serialized to JSON and sent to a GraphQL endpoint. The following example uses RestTemplate with Metatron. !!!caution Don\u2019t forget to set up security groups in Spinnaker! @Metatron(\"spinnaker-app-name-goes-here\") private RestTemplate dgsRestTemplate; private ObjectMapper mapper = new ObjectMapper(); private static HttpEntity<String> httpEntity(String request) { HttpHeaders headers = new HttpHeaders(); headers.setAccept(Collections.singletonList(MediaType.APPLICATION_JSON)); headers.setContentType(MediaType.APPLICATION_JSON); return new HttpEntity<>(request, headers); } Map<String, String> request = Collections.singletonMap(\"query\", graphQLQueryRequest.serialize()); // Invoke REST call, and get the \"ticks\" from data. JsonNode node = dgsRestTemplate.exchange(URL, HttpMethod.POST, httpEntity(mapper.writeValueAsString(request)), new ParameterizedTypeReference<JsonNode>() { }).getBody().get(\"data\").get(\"ticks\"); //Convert to the response type TicksConnection ticks = mapper.convertValue(node, TicksConnection.class); Setting up Client API Code Generation for an external schema To set up client API code generation, first get the server\u2019s schema. If the server\u2019s schema is registered to [Reggie], use the pullSchema Gradle task to accomplish this. This task pulls the latest schema on each build, and thereby makes sure your client is always using an up-to-date schema. To use pullSchema , add the following to your build.gradle : buildscript { dependencies{ classpath 'netflix.studioregistry:netflix.studioregistry.schema-tools:latest.release' } } apply plugin: 'netflix.studioregistry.schema-tools' pullSchema { dgsName = 'your-dgs-spinnaker-name' env = 'test' variant = \"integration\" schemaPath = \"${buildDir}/graphql-schemas/schema.graphqls\" } If the server\u2019s schema is not in Reggie, however, you\u2019ll need to find another way to copy the schema into your client project, and to make sure it stays up-to-date. When you have put the schema file in place, configure code generation by adding the following to your build.gradle : buildscript { dependencies { classpath 'netflix:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'codegen-gradle-plugin' generateJava{ schemaPaths = [\"${buildDir}/graphql-schemas\"] packageName = 'com.netflix.mydgs.generated' // The package name to use to generate sources generateClient = true } //If you are using the pullSchema task to bring in the server schema generateJava.dependsOn(\"pullSchema\") --8<-- \"docs/reference_links\"","title":"Code Generation"},{"location":"generating-code-from-schema/#quick-start","text":"To apply the plugin, update your project\u2019s build.gradle file to include the following: buildscript { dependencies{ classpath 'netflix:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'codegen-gradle-plugin' generateJava{ schemaPaths = [\"${projectDir}/src/main/resources/schema\"] // List of directories containing schema files packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true // Enable generating the type safe query API } The plugin adds a generateJava [Gradle] task that runs as part of your project\u2019s build. generateJava generates the code in the project\u2019s build/generated directory. This folder is automatically added to the project's classpath. Types are available as part of the package specified by the packageName .types , where you specify the value of packageName as a configuration in your build.gradle file. Please ensure that your project\u2019s sources refer to the generated code using the specified package name. generateJava generates the data fetchers and places them in build/generated-examples . !!!note generateJava does not add the data fetchers that it generates to your project\u2019s sources. These fetchers serve mainly as a basic boilerplate code that require further implementation from you. You can exclude parts of the schema from code-generation by placing them in a different schema directory that is not specified as part of the schemaPaths for the plugin.","title":"Quick Start"},{"location":"generating-code-from-schema/#mapping-existing-types","text":"Codegen tries to generate a type for each type it finds in the schema, with a few exceptions. Basic scalar types - are mapped to corresponding Java/Kotlin types (String, Integer etc.) Data and time types - are mapped to corresponding java.time classes PageInfo and RelayPageInfo - are mapped to graphql.relay classes Types mapped with a typeMapping configuration When you have existing classes that you want to use instead of generating a class for a certain type, you can configure the plugin to do so using a typeMapping . The typeMapping configuration is a Map where each key is a GraphQL type and each value is a fully qualified Java/Kotlin type. generateJava{ typeMapping = [\"MyGraphQLType\": \"com.mypackage.MyJavaType\"] }","title":"Mapping existing types"},{"location":"generating-code-from-schema/#generating-client-apis","text":"The code generator can also create client API classes. You can use these classes to query data from a GraphQL endpoint using Java. Java GraphQL clients are useful for server-to-server communication and testing. !!!note Remember that although server-to-server GraphQL is possible, gRPC is a better choice in most cases. !!!note A DGS behind the Studio Edge Gateway should never have to call another DGS behind the gateway directly. Use federation instead! Code generation creates a field-name GraphQLQuery for each Query and Mutation field. The *GraphQLQuery query class contains fields for each parameter of the field. For each type returned by a Query or Mutation, code generation creates a *Projection . A projection is a builder class that specifies which fields get returned. The following is an example usage of a generated API: GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new TicksGraphQLQuery.Builder() .first(first) .after(after) .build(), new TicksConnectionProjection() .edges() .node() .date() .route() .name() .votes() .starRating() .parent() .grade()); This API was generated based on the following schema. The edges and node types are because the schema uses pagination. The API allows for a fluent style of writing queries, with almost the same feel of writing the query as a String, but with the added benefit of code completion and type safety. type Query @extends { ticks(first: Int, after: Int, allowCached: Boolean): TicksConnection } type Tick { id: ID route: Route date: LocalDate userStars: Int userRating: String leadStyle: LeadStyle comments: String } type Votes { starRating: Float nrOfVotes: Int } type Route { routeId: ID name: String grade: String style: Style pitches: Int votes: Votes location: [String] } type TicksConnection { edges: [TickEdge] } type TickEdge { cursor: String! node: Tick }","title":"Generating Client APIs"},{"location":"generating-code-from-schema/#sending-a-query","text":"A GraphQLQueryRequest can be serialized to JSON and sent to a GraphQL endpoint. The following example uses RestTemplate with Metatron. !!!caution Don\u2019t forget to set up security groups in Spinnaker! @Metatron(\"spinnaker-app-name-goes-here\") private RestTemplate dgsRestTemplate; private ObjectMapper mapper = new ObjectMapper(); private static HttpEntity<String> httpEntity(String request) { HttpHeaders headers = new HttpHeaders(); headers.setAccept(Collections.singletonList(MediaType.APPLICATION_JSON)); headers.setContentType(MediaType.APPLICATION_JSON); return new HttpEntity<>(request, headers); } Map<String, String> request = Collections.singletonMap(\"query\", graphQLQueryRequest.serialize()); // Invoke REST call, and get the \"ticks\" from data. JsonNode node = dgsRestTemplate.exchange(URL, HttpMethod.POST, httpEntity(mapper.writeValueAsString(request)), new ParameterizedTypeReference<JsonNode>() { }).getBody().get(\"data\").get(\"ticks\"); //Convert to the response type TicksConnection ticks = mapper.convertValue(node, TicksConnection.class);","title":"Sending a Query"},{"location":"generating-code-from-schema/#setting-up-client-api-code-generation-for-an-external-schema","text":"To set up client API code generation, first get the server\u2019s schema. If the server\u2019s schema is registered to [Reggie], use the pullSchema Gradle task to accomplish this. This task pulls the latest schema on each build, and thereby makes sure your client is always using an up-to-date schema. To use pullSchema , add the following to your build.gradle : buildscript { dependencies{ classpath 'netflix.studioregistry:netflix.studioregistry.schema-tools:latest.release' } } apply plugin: 'netflix.studioregistry.schema-tools' pullSchema { dgsName = 'your-dgs-spinnaker-name' env = 'test' variant = \"integration\" schemaPath = \"${buildDir}/graphql-schemas/schema.graphqls\" } If the server\u2019s schema is not in Reggie, however, you\u2019ll need to find another way to copy the schema into your client project, and to make sure it stays up-to-date. When you have put the schema file in place, configure code generation by adding the following to your build.gradle : buildscript { dependencies { classpath 'netflix:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'codegen-gradle-plugin' generateJava{ schemaPaths = [\"${buildDir}/graphql-schemas\"] packageName = 'com.netflix.mydgs.generated' // The package name to use to generate sources generateClient = true } //If you are using the pullSchema task to bring in the server schema generateJava.dependsOn(\"pullSchema\") --8<-- \"docs/reference_links\"","title":"Setting up Client API Code Generation for an external schema"},{"location":"java-client/","text":"Usage The DGS framework provides a GraphQL client that can be used to retrieve data from a GraphQL endpoint. The client has two components, each usable by itself, or in combination together. GraphQLClient - A HTTP client wrapper that provides easy parsing of GraphQL responses Query API codegen - Generate type-safe Query builders HTTP client wrapper The GraphQL client wraps any HTTP client and provides easy parsing of GraphQL responses. The client can be used against any GraphQL endpoint (it doesn't have to be implemented with the DGS framework), but provides extra conveniences for parsing Gateway and DGS responses. This includes support for the Errors Spec . To use the client, add the com.netflix.graphql.dgs:graphql-dgs-client:latest.release dependency to build.gradle and create an instance of DefaultGraphQLClient . GraphQLClient client = new DefaultGraphQLClient(url); The url is the server url of the endpoint you want to call. This url will be passed down to the callback discussed below. !!!info Note that for NIWS clients there typically is no url, because the endpoint is configured through properties. Just pass an empty string in this case. Using the GraphQLClient a query can be executed. The executeQuery method has three arguments: The query String An optional map of query variables An instance of RequestExecutor , typically provided as a lambda. Because of the large number HTTP clients in use within Netflix, the GraphQLClient is decoupled from any particular HTTP client implementation. Any HTTP client (RestTemplate, RestClient, NIWS, OkHTTP, ....) can be used. The developer is responsible for making the actual HTTP call by implementing a RequestExecutor . RequestExecutor receives the url , a map of headers and the request body as parameters, and should return an instance of HttpResponse . Based on the HTTP response the GraphQLClient parses the response and provides easy access to data and errors. The example below uses RestTemplate with Metatron. @Metatron(\"mountainprojectdgs\") private RestTemplate dgsRestTemplate; private static final String URL = \"https://mountainprojectdgs.cluster.us-east-1.test.cloud.netflix.net:8443/graphql\"; private static final String QUERY = \"{\\n\" + \" ticks(first: %d, after:%d){\\n\" + \" edges {\\n\" + \" node {\\n\" + \" route {\\n\" + \" name\\n\" + \" grade\\n\" + \" pitches\\n\" + \" location\\n\" + \" }\\n\" + \" \\n\" + \" userStars\\n\" + \" }\\n\" + \" }\\n\" + \" }\\n\" + \"}\"; public List<TicksConnection> getData() { DefaultGraphQLClient graphQLClient = new DefaultGraphQLClient(URL); GraphQLResponse response = graphQLClient.executeQuery(query, new HashMap<>(), (url, headers, body) -> { /** * The requestHeaders providers headers typically required to call a GraphQL endpoint, including the Accept and Content-Type headers. * To use RestTemplate, the requestHeaders need to be transformed into Spring's HttpHeaders. */ HttpHeaders requestHeaders = new HttpHeaders(); headers.forEach(requestHeaders::put); /** * Use RestTemplate to call the GraphQL service. * The response type should simply be String, because the parsing will be done by the GraphQLClient. */ ResponseEntity<String> exchange = dgsRestTemplate.exchange(url, HttpMethod.POST, new HttpEntity(body, requestHeaders), String.class); /** * Return a HttpResponse, which contains the HTTP status code and response body (as a String). * The way to get these depend on the HTTP client. */ return new HttpResponse(exchange.getStatusCodeValue(), exchange.getBody()); }); TicksConnection ticks = graphQLResponse.extractValueAsObject(\"ticks\", TicksConnection.class); return ticks; } The GraphQLClient provides methods to parse and retrieve data and errors in a variety of ways. Refer to the GrqphQLClient JavaDoc for the complete list of supported methods. method description example getData Get the data as a Map Map<String,Object> data = response.getData() dataAsObject Parse data as the provided class, using the Jackson Object Mapper TickResponse data = response.dataAsObject(TicksResponse.class) extractValue Extract values given a JsonPath . The return type will be whatever type you expect, but depends on the JSON shape. For JSON objects, a Map is returned. Although this looks type safe, it really isn't. It's mostly useful for \"simple\" types like String, Int etc., and Lists of those types. List<String> name = response.extractValue(\"movies[*].originalTitle\") extractValueAsObject Extract values given a JsonPath and deserialize into the given class Ticks ticks = response.extractValueAsObject(\"ticks\", Ticks.class) extractValueAsObject Extract values given a JsonPath and deserialize into the given TypeRef. Useful for Maps and Lists of a certain class. List<Route> routes = response.extractValueAsObject(\"ticks.edges[*].node.route\", new TypeRef<List<Route>>(){}) getRequestDetails Extract a RequestDetails object. This only works if requestDetails was requested in the query, and against the Gateway. RequestDetails requestDetails = response.getRequestDetails() getParsed Get the parsed DocumentContext for further JsonPath processing response.getDocumentContext() Errors The GraphQLClient checks both for HTTP level errors (based on the response status code) and the errors block in a GraphQL response. The GraphQLClient is compatible with the Errors Spec used by the Gateway and DGS, and makes it easy to extract error information such as the ErrorType. For example, for following GraphQL response the GraphQLClient lets you easily get the ErrorType and ErrorDetail fields. Note that the ErrorType is an enum as specified by the Errors Spec . { \"errors\": [ { \"message\": \"java.lang.RuntimeException: test\", \"locations\": [], \"path\": [ \"hello\" ], \"extensions\": { \"errorType\": \"BAD_REQUEST\", \"errorDetail\": \"FIELD_NOT_FOUND\" } } ], \"data\": { \"hello\": null } } assertThat(graphQLResponse.errors.get(0).extensions.errorType).isEqualTo(ErrorType.BAD_REQUEST) assertThat(graphQLResponse.errors.get(0).extensions.errorDetail).isEqualTo(\"FIELD_NOT_FOUND\") Type safe Query API Based on a GraphQL schema a type safe query API can be generated for Java/Kotlin. The generated API is a builder style API that lets you build a GraphQL query and it's projection (field selection). Because the code gets re-generated when the schema changes, it helps catch errors in the query. Because Java doesn't support multi-line strings (yet) it's also arguably a more readable way to specify a query. If you own a DGS and want to generate a client for this DGS (e.g. for testing purposes) the client generation is just an extra property on the Codegen configuration . Specify the following in your build.gradle . buildscript { dependencies{ classpath 'netflix:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'codegen-gradle-plugin' generateJava{ packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true } Code will be generated on build. The generated code is in build/generated . NOTE: Make sure the codegen plugin is the first plugin on the build script class path. One of the grpc plugins pulls in an old version of ANTLR. If you want to generate a client for another DGS/GraphQL service you'll need to pull the service's schema somehow. If the service is a DGS registered in Reggie, the DGS schema tools can be used. buildscript { dependencies { classpath 'netflix.studioregistry:netflix.studioregistry.schema-tools:latest.release' classpath 'netflix:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'netflix.studioregistry.schema-tools' apply plugin: 'codegen-gradle-plugin' pullSchema { dgsName = 'mountainprojectdgs' env = 'test' variant = \"integration\" schemaPath = \"${buildDir}/graphql-schemas/mountainproject.graphqls\" } generateJava{ schemaPaths = [\"${buildDir}/graphql-schemas\"] packageName = 'com.netflix.mountainprojectdgsclient.generated' // The package name to use to generate sources typeMapping = [\"LocalDate\": \"java.lang.String\"] generateClient = true } generateJava.dependsOn(\"pullSchema\") With the configuration above, the schema will be pulled from Reggie into the build directory, and code will be generated from that schema. This is great to assure that the generated code stays in sync with the schema. If the service is a stand-alone DGS, or not a DGS at all, you will have to manually copy the schema to your project and keep it in sync. With codegen configured correctly, a builder style API will be generated when building the project. Using the same query example as above, the query can be build using the generated builder API. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new TicksGraphQLQuery.Builder() .first(first) .after(after) .build(), new TicksConnectionProjectionRoot() .edges() .node() .date() .route() .name() .votes() .starRating() .parent() .grade()); String query = graphQLQueryRequest.serialize(); The GraphQLQueryRequest is a class from graphql-dgs-client . The TicksGraphQLQuery and TicksConnectionProjectionRoot are generated. After building the query, it can be serialized to a String, and executed using the GraphQLClient. Note that the edges and node fields are because the example schema is using Relay pagination. Interface projections When a field returns an interface, fields on the concrete types are specified using a fragment. type Query @extends { script(name: String): Script } interface Script { title: String director: String actors: [Actor] } type MovieScript implements Script { title: String director: String length: Int } type ShowScript implements Script { title: String director: String episodes: Int } query { script(name: \"Top Secret\") { title ... on MovieScript { length } } } This syntax is supported by the Query builder as well. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new ScriptGraphQLQuery.Builder() .name(\"Top Secret\") .build(), new ScriptProjectionRoot() .title() .onMovieScript() .length(); ); Building Federated Queries You can use GraphQLQueryRequest along with EntitiesGraphQLQuery to generated federated queries. The API provides a type-safe way to construct the _entities query with the associated representations based on the input schema. The representations are passed in as a map of variables. Each representation class is generated based on the key fields defined on the entity in your schema, along with the __typename . The EntitiesProjectionRoot is used to select query fields on the specified type. For example, let us look at a schema that extends a Movie type: type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } With client code generation, you will now have a MovieRepresentation containing the key field, i.e., movieId , and the __typename field already set to type Movie . Now you can add each representation to the EntitiesGraphQLQuery as a representations variable. You will also have a EntitiesProjectionRoot with onMovie() to select fields on Movie from. Finally, you put them all together as a GraphQLQueryRequest , which you serialize into the final query string. The map of representations variables is available via getVariables on the EntitiesGraphQLQuery . Here is an example for the schema shown earlier: EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery.Builder() .addRepresentationAsVariable( MovieRepresentation.newBuilder().movieId(1122).build() ) .build(); GraphQLQueryRequest request = new GraphQLQueryRequest( entitiesQuery, new EntitiesProjectionRoot().onMovie().movieId().script().title() ); String query = request.serialize(); Map<String, Object> representations = entitiesQuery.getVariables();","title":"Java GraphQL Client"},{"location":"java-client/#usage","text":"The DGS framework provides a GraphQL client that can be used to retrieve data from a GraphQL endpoint. The client has two components, each usable by itself, or in combination together. GraphQLClient - A HTTP client wrapper that provides easy parsing of GraphQL responses Query API codegen - Generate type-safe Query builders","title":"Usage"},{"location":"java-client/#http-client-wrapper","text":"The GraphQL client wraps any HTTP client and provides easy parsing of GraphQL responses. The client can be used against any GraphQL endpoint (it doesn't have to be implemented with the DGS framework), but provides extra conveniences for parsing Gateway and DGS responses. This includes support for the Errors Spec . To use the client, add the com.netflix.graphql.dgs:graphql-dgs-client:latest.release dependency to build.gradle and create an instance of DefaultGraphQLClient . GraphQLClient client = new DefaultGraphQLClient(url); The url is the server url of the endpoint you want to call. This url will be passed down to the callback discussed below. !!!info Note that for NIWS clients there typically is no url, because the endpoint is configured through properties. Just pass an empty string in this case. Using the GraphQLClient a query can be executed. The executeQuery method has three arguments: The query String An optional map of query variables An instance of RequestExecutor , typically provided as a lambda. Because of the large number HTTP clients in use within Netflix, the GraphQLClient is decoupled from any particular HTTP client implementation. Any HTTP client (RestTemplate, RestClient, NIWS, OkHTTP, ....) can be used. The developer is responsible for making the actual HTTP call by implementing a RequestExecutor . RequestExecutor receives the url , a map of headers and the request body as parameters, and should return an instance of HttpResponse . Based on the HTTP response the GraphQLClient parses the response and provides easy access to data and errors. The example below uses RestTemplate with Metatron. @Metatron(\"mountainprojectdgs\") private RestTemplate dgsRestTemplate; private static final String URL = \"https://mountainprojectdgs.cluster.us-east-1.test.cloud.netflix.net:8443/graphql\"; private static final String QUERY = \"{\\n\" + \" ticks(first: %d, after:%d){\\n\" + \" edges {\\n\" + \" node {\\n\" + \" route {\\n\" + \" name\\n\" + \" grade\\n\" + \" pitches\\n\" + \" location\\n\" + \" }\\n\" + \" \\n\" + \" userStars\\n\" + \" }\\n\" + \" }\\n\" + \" }\\n\" + \"}\"; public List<TicksConnection> getData() { DefaultGraphQLClient graphQLClient = new DefaultGraphQLClient(URL); GraphQLResponse response = graphQLClient.executeQuery(query, new HashMap<>(), (url, headers, body) -> { /** * The requestHeaders providers headers typically required to call a GraphQL endpoint, including the Accept and Content-Type headers. * To use RestTemplate, the requestHeaders need to be transformed into Spring's HttpHeaders. */ HttpHeaders requestHeaders = new HttpHeaders(); headers.forEach(requestHeaders::put); /** * Use RestTemplate to call the GraphQL service. * The response type should simply be String, because the parsing will be done by the GraphQLClient. */ ResponseEntity<String> exchange = dgsRestTemplate.exchange(url, HttpMethod.POST, new HttpEntity(body, requestHeaders), String.class); /** * Return a HttpResponse, which contains the HTTP status code and response body (as a String). * The way to get these depend on the HTTP client. */ return new HttpResponse(exchange.getStatusCodeValue(), exchange.getBody()); }); TicksConnection ticks = graphQLResponse.extractValueAsObject(\"ticks\", TicksConnection.class); return ticks; } The GraphQLClient provides methods to parse and retrieve data and errors in a variety of ways. Refer to the GrqphQLClient JavaDoc for the complete list of supported methods. method description example getData Get the data as a Map Map<String,Object> data = response.getData() dataAsObject Parse data as the provided class, using the Jackson Object Mapper TickResponse data = response.dataAsObject(TicksResponse.class) extractValue Extract values given a JsonPath . The return type will be whatever type you expect, but depends on the JSON shape. For JSON objects, a Map is returned. Although this looks type safe, it really isn't. It's mostly useful for \"simple\" types like String, Int etc., and Lists of those types. List<String> name = response.extractValue(\"movies[*].originalTitle\") extractValueAsObject Extract values given a JsonPath and deserialize into the given class Ticks ticks = response.extractValueAsObject(\"ticks\", Ticks.class) extractValueAsObject Extract values given a JsonPath and deserialize into the given TypeRef. Useful for Maps and Lists of a certain class. List<Route> routes = response.extractValueAsObject(\"ticks.edges[*].node.route\", new TypeRef<List<Route>>(){}) getRequestDetails Extract a RequestDetails object. This only works if requestDetails was requested in the query, and against the Gateway. RequestDetails requestDetails = response.getRequestDetails() getParsed Get the parsed DocumentContext for further JsonPath processing response.getDocumentContext()","title":"HTTP client wrapper"},{"location":"java-client/#errors","text":"The GraphQLClient checks both for HTTP level errors (based on the response status code) and the errors block in a GraphQL response. The GraphQLClient is compatible with the Errors Spec used by the Gateway and DGS, and makes it easy to extract error information such as the ErrorType. For example, for following GraphQL response the GraphQLClient lets you easily get the ErrorType and ErrorDetail fields. Note that the ErrorType is an enum as specified by the Errors Spec . { \"errors\": [ { \"message\": \"java.lang.RuntimeException: test\", \"locations\": [], \"path\": [ \"hello\" ], \"extensions\": { \"errorType\": \"BAD_REQUEST\", \"errorDetail\": \"FIELD_NOT_FOUND\" } } ], \"data\": { \"hello\": null } } assertThat(graphQLResponse.errors.get(0).extensions.errorType).isEqualTo(ErrorType.BAD_REQUEST) assertThat(graphQLResponse.errors.get(0).extensions.errorDetail).isEqualTo(\"FIELD_NOT_FOUND\")","title":"Errors"},{"location":"java-client/#type-safe-query-api","text":"Based on a GraphQL schema a type safe query API can be generated for Java/Kotlin. The generated API is a builder style API that lets you build a GraphQL query and it's projection (field selection). Because the code gets re-generated when the schema changes, it helps catch errors in the query. Because Java doesn't support multi-line strings (yet) it's also arguably a more readable way to specify a query. If you own a DGS and want to generate a client for this DGS (e.g. for testing purposes) the client generation is just an extra property on the Codegen configuration . Specify the following in your build.gradle . buildscript { dependencies{ classpath 'netflix:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'codegen-gradle-plugin' generateJava{ packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true } Code will be generated on build. The generated code is in build/generated . NOTE: Make sure the codegen plugin is the first plugin on the build script class path. One of the grpc plugins pulls in an old version of ANTLR. If you want to generate a client for another DGS/GraphQL service you'll need to pull the service's schema somehow. If the service is a DGS registered in Reggie, the DGS schema tools can be used. buildscript { dependencies { classpath 'netflix.studioregistry:netflix.studioregistry.schema-tools:latest.release' classpath 'netflix:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'netflix.studioregistry.schema-tools' apply plugin: 'codegen-gradle-plugin' pullSchema { dgsName = 'mountainprojectdgs' env = 'test' variant = \"integration\" schemaPath = \"${buildDir}/graphql-schemas/mountainproject.graphqls\" } generateJava{ schemaPaths = [\"${buildDir}/graphql-schemas\"] packageName = 'com.netflix.mountainprojectdgsclient.generated' // The package name to use to generate sources typeMapping = [\"LocalDate\": \"java.lang.String\"] generateClient = true } generateJava.dependsOn(\"pullSchema\") With the configuration above, the schema will be pulled from Reggie into the build directory, and code will be generated from that schema. This is great to assure that the generated code stays in sync with the schema. If the service is a stand-alone DGS, or not a DGS at all, you will have to manually copy the schema to your project and keep it in sync. With codegen configured correctly, a builder style API will be generated when building the project. Using the same query example as above, the query can be build using the generated builder API. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new TicksGraphQLQuery.Builder() .first(first) .after(after) .build(), new TicksConnectionProjectionRoot() .edges() .node() .date() .route() .name() .votes() .starRating() .parent() .grade()); String query = graphQLQueryRequest.serialize(); The GraphQLQueryRequest is a class from graphql-dgs-client . The TicksGraphQLQuery and TicksConnectionProjectionRoot are generated. After building the query, it can be serialized to a String, and executed using the GraphQLClient. Note that the edges and node fields are because the example schema is using Relay pagination.","title":"Type safe Query API"},{"location":"java-client/#interface-projections","text":"When a field returns an interface, fields on the concrete types are specified using a fragment. type Query @extends { script(name: String): Script } interface Script { title: String director: String actors: [Actor] } type MovieScript implements Script { title: String director: String length: Int } type ShowScript implements Script { title: String director: String episodes: Int } query { script(name: \"Top Secret\") { title ... on MovieScript { length } } } This syntax is supported by the Query builder as well. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new ScriptGraphQLQuery.Builder() .name(\"Top Secret\") .build(), new ScriptProjectionRoot() .title() .onMovieScript() .length(); );","title":"Interface projections"},{"location":"java-client/#building-federated-queries","text":"You can use GraphQLQueryRequest along with EntitiesGraphQLQuery to generated federated queries. The API provides a type-safe way to construct the _entities query with the associated representations based on the input schema. The representations are passed in as a map of variables. Each representation class is generated based on the key fields defined on the entity in your schema, along with the __typename . The EntitiesProjectionRoot is used to select query fields on the specified type. For example, let us look at a schema that extends a Movie type: type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } With client code generation, you will now have a MovieRepresentation containing the key field, i.e., movieId , and the __typename field already set to type Movie . Now you can add each representation to the EntitiesGraphQLQuery as a representations variable. You will also have a EntitiesProjectionRoot with onMovie() to select fields on Movie from. Finally, you put them all together as a GraphQLQueryRequest , which you serialize into the final query string. The map of representations variables is available via getVariables on the EntitiesGraphQLQuery . Here is an example for the schema shown earlier: EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery.Builder() .addRepresentationAsVariable( MovieRepresentation.newBuilder().movieId(1122).build() ) .build(); GraphQLQueryRequest request = new GraphQLQueryRequest( entitiesQuery, new EntitiesProjectionRoot().onMovie().movieId().script().title() ); String query = request.serialize(); Map<String, Object> representations = entitiesQuery.getVariables();","title":"Building Federated Queries"},{"location":"metrics-tracing/","text":"The [DGS] framework provides rich support for distributed tracing and logging, out of the box. Query Logging By default, every [DGS] app publishes query logs to [Elasticsearch]. An Elasticsearch cluster contains all query logs of all DGSs. You do not need to do any additional configuration in order to set this up. You can use Kibana to view/search the logs: Kibana in Test Kibana in Prod If your service contains sensitive data that should not be visible in logs, you have two options: Use the framework\u2019s log sanitizer to strip out query input variables and query result values. Set dgs.logging.sanitize to true . Disable query logging entirely. Set dgs.logging.keystone.enabled to false . Edgar Integration [Edgar] is a tool that enables request tracing. In the context of Studio Edge, it creates a trace from the [Studio Edge Gateway] to the different [DGS]s that the gateway uses to resolve a query. For each DGS it shows which data fetchers were used , including timing information. For each DGS call it also links the query logs (from [Elasticsearch]) to each trace. You can use Edgar to debug issues for specific (types of) requests and to understand how systems are connected to each other. Note that Edgar even captures traces for DGS instances running on localhost, which can be a great tool during development. !!!abstract \"Example\" The following is an example of an Edgar trace for a query that was resolved by the Headshot DGS and [Maple]. ![Edgar trace](../../img/edgar-trace1.png) ![Edgar trace](../../img/edgar-trace2.png) Edgar Setup To enable [Edgar] for a new [DGS], provide two pieces of configuration: First, set the following configuration in your application.yml : spring: sleuth: sampler: probability: 1.0 !!!note If you register your Studio Edge [DGS] by means of the Reggie tool, that tool will configure Edgar for you automatically and you do not need to take the manual steps described to set up with Edgar UI. If you did not register your application through reggie, register your application in Edgar by using its self service UI . property value Application name Spinnaker app name Source name dgs_logging Hostname es_tribe_dgs Indices dgs_logging* Log Type DgsLog Span ID spanId Trace ID traceId Timestamp field ts Make sure to register both for PROD and for TEST. !!!note You do not have to set up security groups (as indicated by the UI); this is already done. Available Spans The DGS framework creates several spans by default for each request. span ID description post /graphql This span is created by Spring Boot before the DGS framework starts processing the request. Any time between this span and the dgs.graphqlendpoint span are typically filters. dgs.graphqlendpoint Starts when the DGS framework starts processing the request, and includes DGS authz checks. dgs.queryexecution Starts when the query is executed, after input is parsed and context is prepared. dgs.datafetcher. parentType . field Invocation of a datafetcher. The name of the span includes the parentType and field as specified in @DgsData . See the next section about async datafetchers. dgs.dataloader. name Invocation of a dataloader. The name of the span includes the name of the dataloader. Datafetcher and DataLoader Tracing By default, the [DGS] framework creates a span for each datafetcher invocation, unless the datafetcher returns a CompletableFuture or CompletionStage . Although not always true, the common case for a datafetcher to return CompletableFuture or CompletionStage is when the datafetcher uses a DataLoader. In the case of batch loading, the datafetcher might be called many times, while the result is only a single invocation to a DataLoader. In this scenario there isn\u2019t much value in creating spans for each individual datafetcher invocation ; instead we just want to see the time spent by the DataLoader. For this reason, the DGS framework will create no span for datafetchers returning CompletableFuture or CompletionStage . However, you can override this behavior by using the @DgsEnableDataFetcherTracing annotation. You can use the same annotation to explicitly disable span creation for a datafetcher. To do so, pass false as an argument to the annotation: @DgsData(parentType=\"Query\", field=\"someField\") @DgsEnableDataFetcherTracing(false) public String someDataFetcher() { ... } This is useful for datafetchers that get in-memory data for a specific field. Cross Region Requests [Edgar] keeps track of requests both from a client and server perspective. A trace starts when a HTTP or [gRPC] client starts a connection. Another trace starts when the server receives the request and starts processing it. There can be significant time (up to ~500ms) spent on getting the request from a client to a server, this is specially true when cross region calls are involved . Other factors are \u201cwarming\u201d of a client and its connection pools. Edgar displays the client portion of a request in a lighter color. It\u2019s important to understand this difference to know where to look for optimizations. Atlas Metrics Aside from the many metrics already provided out-of-the-box with [Spring Boot], the [DGS] framework adds metrics and tracing. These metrics work out-of-the-box and don\u2019t require any configuration. IPC Metrics with graphql tags As mentioned earlier, all DGSs have the standard ipc metrics that come with Spring Boot apps. The server call related metrics (ipc.server.call and ipc.server.inflight) also have the following tags for GraphQL queries: Tags: tag name description ipc.protocol Set to graphql for GraphQL requests. ipc.status Set to success with most GraphQL requests, except Internal Server Error or Bad request. ipc.result Set to success or failure for GraphQL requests. ipc.status.detail Set to gql_error for GraphQL requests. gql.queryComplexity The query complexity computed by graphql-java instrumentation using buckets 5, 10, 25, 50, 100, 200, 500, 1000, 2000, 5000, 10000. !!!info \"Future Enhancements\" In the future, the following IPC metric tags will also available: ipc.client.app , ipc.client.cluster , ipc.client.asg , ipc.client.region , and ipc.client.zone . These are currently not propagated by the gateway, but will be supported in the future. Example query: nf.app,requestdetailsdgs,:eq, name,ipc.server.call,:eq,:and, ipc.protocol,graphql,:eq,:and, gql.queryComplexity,3,:eq,:and, :dist-avg Error Counter The error counter counts the number of GraphQL errors encountered during query execution. Name: gql.error Type: Counter Tags: tag name description gql.errorCode The GraphQL error code, such as VALIDATION , INTERNAL , etc. gql.errorPath The sanitized query path that resulted in the error. gql.errorDetail Optional flag containing additional details, if present. Example query: nf.app,mountainprojectdgs,:eq, name,gql.error,:eq,:and, :sum, (,gql.errorCode,),:by Data Loader Timer The data loader timer times the execution time for the data loader invocation for a batch of queries. This is useful if you want to find data loaders that might be responsible for poor query performance. Name: gql.dataLoader Type: Timer Tags: tag name description gql.loaderName The name of the data loader, may or may not be the same as the type of entity. gql.loaderBatchSize The number of queries executed in the batch. Example query: nf.app,mountainprojectdgs,:eq, name,gql.dataLoader,:eq,:and, :max, (,gql.loaderName,),:by Data Fetcher Counter The data fetcher or resolver counter counts the number of invocations of each data fetcher. This is useful if you want to find out which data fetchers are used often and which ones aren\u2019t used at all. Note that this is not available if used with a batch loader. Name: gql.resolver.count Type: Counter Tags: tag name description gql.field Name of the data fetcher. This has the ${parentType}.${field} format as specified in the @DgsData annotation. Example query: nf.app,requestdetailsdgs,:eq, name,gql.resolver.count,:eq,:and, :sum, (,gql.field,),:by Data Fetcher Timer The data fetcher timer times the execution time for each data fetcher invocation. This is useful if you want to find data fetchers that might be responsible for poor query performance. Note that this metric is not available if used with a batch loader. Name: gql.resolver.time Type: Timer Tags: tag name description gql.field Name of the data fetcher. This has the ${parentType}.${field} format as specified in the @DgsData annotation. Example query: nf.app,requestdetailsdgs,:eq, name,gql.resolver.time,:eq,:and, :dist-avg, (,gql.field,),:by --8<-- \"docs/reference_links\"","title":"Metrics tracing"},{"location":"metrics-tracing/#query-logging","text":"By default, every [DGS] app publishes query logs to [Elasticsearch]. An Elasticsearch cluster contains all query logs of all DGSs. You do not need to do any additional configuration in order to set this up. You can use Kibana to view/search the logs: Kibana in Test Kibana in Prod If your service contains sensitive data that should not be visible in logs, you have two options: Use the framework\u2019s log sanitizer to strip out query input variables and query result values. Set dgs.logging.sanitize to true . Disable query logging entirely. Set dgs.logging.keystone.enabled to false .","title":"Query Logging"},{"location":"metrics-tracing/#edgar-integration","text":"[Edgar] is a tool that enables request tracing. In the context of Studio Edge, it creates a trace from the [Studio Edge Gateway] to the different [DGS]s that the gateway uses to resolve a query. For each DGS it shows which data fetchers were used , including timing information. For each DGS call it also links the query logs (from [Elasticsearch]) to each trace. You can use Edgar to debug issues for specific (types of) requests and to understand how systems are connected to each other. Note that Edgar even captures traces for DGS instances running on localhost, which can be a great tool during development. !!!abstract \"Example\" The following is an example of an Edgar trace for a query that was resolved by the Headshot DGS and [Maple]. ![Edgar trace](../../img/edgar-trace1.png) ![Edgar trace](../../img/edgar-trace2.png)","title":"Edgar Integration"},{"location":"metrics-tracing/#edgar-setup","text":"To enable [Edgar] for a new [DGS], provide two pieces of configuration: First, set the following configuration in your application.yml : spring: sleuth: sampler: probability: 1.0 !!!note If you register your Studio Edge [DGS] by means of the Reggie tool, that tool will configure Edgar for you automatically and you do not need to take the manual steps described to set up with Edgar UI. If you did not register your application through reggie, register your application in Edgar by using its self service UI . property value Application name Spinnaker app name Source name dgs_logging Hostname es_tribe_dgs Indices dgs_logging* Log Type DgsLog Span ID spanId Trace ID traceId Timestamp field ts Make sure to register both for PROD and for TEST. !!!note You do not have to set up security groups (as indicated by the UI); this is already done.","title":"Edgar Setup"},{"location":"metrics-tracing/#available-spans","text":"The DGS framework creates several spans by default for each request. span ID description post /graphql This span is created by Spring Boot before the DGS framework starts processing the request. Any time between this span and the dgs.graphqlendpoint span are typically filters. dgs.graphqlendpoint Starts when the DGS framework starts processing the request, and includes DGS authz checks. dgs.queryexecution Starts when the query is executed, after input is parsed and context is prepared. dgs.datafetcher. parentType . field Invocation of a datafetcher. The name of the span includes the parentType and field as specified in @DgsData . See the next section about async datafetchers. dgs.dataloader. name Invocation of a dataloader. The name of the span includes the name of the dataloader.","title":"Available Spans"},{"location":"metrics-tracing/#datafetcher-and-dataloader-tracing","text":"By default, the [DGS] framework creates a span for each datafetcher invocation, unless the datafetcher returns a CompletableFuture or CompletionStage . Although not always true, the common case for a datafetcher to return CompletableFuture or CompletionStage is when the datafetcher uses a DataLoader. In the case of batch loading, the datafetcher might be called many times, while the result is only a single invocation to a DataLoader. In this scenario there isn\u2019t much value in creating spans for each individual datafetcher invocation ; instead we just want to see the time spent by the DataLoader. For this reason, the DGS framework will create no span for datafetchers returning CompletableFuture or CompletionStage . However, you can override this behavior by using the @DgsEnableDataFetcherTracing annotation. You can use the same annotation to explicitly disable span creation for a datafetcher. To do so, pass false as an argument to the annotation: @DgsData(parentType=\"Query\", field=\"someField\") @DgsEnableDataFetcherTracing(false) public String someDataFetcher() { ... } This is useful for datafetchers that get in-memory data for a specific field.","title":"Datafetcher and DataLoader Tracing"},{"location":"metrics-tracing/#cross-region-requests","text":"[Edgar] keeps track of requests both from a client and server perspective. A trace starts when a HTTP or [gRPC] client starts a connection. Another trace starts when the server receives the request and starts processing it. There can be significant time (up to ~500ms) spent on getting the request from a client to a server, this is specially true when cross region calls are involved . Other factors are \u201cwarming\u201d of a client and its connection pools. Edgar displays the client portion of a request in a lighter color. It\u2019s important to understand this difference to know where to look for optimizations.","title":"Cross Region Requests"},{"location":"metrics-tracing/#atlas-metrics","text":"Aside from the many metrics already provided out-of-the-box with [Spring Boot], the [DGS] framework adds metrics and tracing. These metrics work out-of-the-box and don\u2019t require any configuration.","title":"Atlas Metrics"},{"location":"metrics-tracing/#ipc-metrics-with-graphql-tags","text":"As mentioned earlier, all DGSs have the standard ipc metrics that come with Spring Boot apps. The server call related metrics (ipc.server.call and ipc.server.inflight) also have the following tags for GraphQL queries: Tags: tag name description ipc.protocol Set to graphql for GraphQL requests. ipc.status Set to success with most GraphQL requests, except Internal Server Error or Bad request. ipc.result Set to success or failure for GraphQL requests. ipc.status.detail Set to gql_error for GraphQL requests. gql.queryComplexity The query complexity computed by graphql-java instrumentation using buckets 5, 10, 25, 50, 100, 200, 500, 1000, 2000, 5000, 10000. !!!info \"Future Enhancements\" In the future, the following IPC metric tags will also available: ipc.client.app , ipc.client.cluster , ipc.client.asg , ipc.client.region , and ipc.client.zone . These are currently not propagated by the gateway, but will be supported in the future. Example query: nf.app,requestdetailsdgs,:eq, name,ipc.server.call,:eq,:and, ipc.protocol,graphql,:eq,:and, gql.queryComplexity,3,:eq,:and, :dist-avg","title":"IPC Metrics with graphql tags"},{"location":"metrics-tracing/#error-counter","text":"The error counter counts the number of GraphQL errors encountered during query execution. Name: gql.error Type: Counter Tags: tag name description gql.errorCode The GraphQL error code, such as VALIDATION , INTERNAL , etc. gql.errorPath The sanitized query path that resulted in the error. gql.errorDetail Optional flag containing additional details, if present. Example query: nf.app,mountainprojectdgs,:eq, name,gql.error,:eq,:and, :sum, (,gql.errorCode,),:by","title":"Error Counter"},{"location":"metrics-tracing/#data-loader-timer","text":"The data loader timer times the execution time for the data loader invocation for a batch of queries. This is useful if you want to find data loaders that might be responsible for poor query performance. Name: gql.dataLoader Type: Timer Tags: tag name description gql.loaderName The name of the data loader, may or may not be the same as the type of entity. gql.loaderBatchSize The number of queries executed in the batch. Example query: nf.app,mountainprojectdgs,:eq, name,gql.dataLoader,:eq,:and, :max, (,gql.loaderName,),:by","title":"Data Loader Timer"},{"location":"metrics-tracing/#data-fetcher-counter","text":"The data fetcher or resolver counter counts the number of invocations of each data fetcher. This is useful if you want to find out which data fetchers are used often and which ones aren\u2019t used at all. Note that this is not available if used with a batch loader. Name: gql.resolver.count Type: Counter Tags: tag name description gql.field Name of the data fetcher. This has the ${parentType}.${field} format as specified in the @DgsData annotation. Example query: nf.app,requestdetailsdgs,:eq, name,gql.resolver.count,:eq,:and, :sum, (,gql.field,),:by","title":"Data Fetcher Counter"},{"location":"metrics-tracing/#data-fetcher-timer","text":"The data fetcher timer times the execution time for each data fetcher invocation. This is useful if you want to find data fetchers that might be responsible for poor query performance. Note that this metric is not available if used with a batch loader. Name: gql.resolver.time Type: Timer Tags: tag name description gql.field Name of the data fetcher. This has the ${parentType}.${field} format as specified in the @DgsData annotation. Example query: nf.app,requestdetailsdgs,:eq, name,gql.resolver.time,:eq,:and, :dist-avg, (,gql.field,),:by --8<-- \"docs/reference_links\"","title":"Data Fetcher Timer"},{"location":"mutations/","text":"The DGS framework supports Mutations with the same constructs as data fetchers, using the @DgsData annotation. The following is a simple example of a mutation: type Mutation { addRating(title: String, stars: Int):Rating } type Rating { avgStars: Float } @DgsComponent public class RatingMutation { @DgsData(parentType = \"Mutation\", field = \"addRating\") public Rating addRating(DataFetchingEnvironment dataFetchingEnvironment) { int stars = dataFetchingEnvironment.getArgument(\"stars\"); if(stars < 1) { throw new IllegalArgumentException(\"Stars must be 1-5\"); } String title = dataFetchingEnvironment.getArgument(\"title\"); System.out.println(\"Rated \" + title + \" with \" + stars + \" stars\") ; return new Rating(stars); } } Note that the code above retrieves the input data for the Mutation by calling the DataFetchingEnvironment.getArgument method, just as data fetchers do for their arguments. Input Types In the example above the input was two standard scalar types. You can also use complex types, and you should define these as input types in your schema. An input type is almost the same as a type in [GraphQL], but with some extra rules . According to the GraphQL specification an input type should always be passed to the data fetcher as a Map . This means the DataFetchingEnvironment.getArgument for an input type is a Map , and not the Java/Kotlin representation that you might have. The framework has a convenience mechanism around this, which will be discussed next. Let's first look at an example that uses DataFetchingEnvironment directly. type Mutation { addRating(input: RatingInput):Rating } input RatingInput { title: String, stars: Int } type Rating { avgStars: Float } @DgsComponent public class RatingMutation { @DgsData(parentType = \"Mutation\", field = \"addRating\") public Rating addRating(DataFetchingEnvironment dataFetchingEnvironment) { Map<String,Object> input = dataFetchingEnvironment.getArgument(\"input\"); RatingInput ratingInput = new ObjectMapper().convertValue(input, RatingInput.class); System.out.println(\"Rated \" + ratingInput.getTitle() + \" with \" + ratingInput.getStars() + \" stars\") ; return new Rating(ratingInput.getStars()); } } class RatingInput { private String title; private int stars; public String getTitle() { return title; } public void setTitle(String title) { this.title = title; } public int getStars() { return stars; } public void setStars(int stars) { this.stars = stars; } } Input arguments as data fetchter method parameters The framework makes it easier to get input arguments. You can specify arguments as method parameters of a data fetcher. @DgsComponent public class RatingMutation { @DgsData(parentType = \"Mutation\", field = \"addRating\") public Rating addRating(@InputArgument(\"input\") RatingInput ratingInput) { //No need for custom parsing anymore! System.out.println(\"Rated \" + ratingInput.getTitle() + \" with \" + ratingInput.getStars() + \" stars\") ; return new Rating(ratingInput.getStars()); } } The @InputArgument annotation is important to specify the name of the input argument, because arguments can be specified in any order. If no annotation is present, the framework tries to use the parameter name, but this is only possible if the code is compiled with specific compiler settings . Input type parameters can be combined with a DataFetchingEnvironment parameter. @DgsComponent public class RatingMutation { @DgsData(parentType = \"Mutation\", field = \"addRating\") public Rating addRating(@InputArgument(\"input\") RatingInput ratingInput, DataFetchingEnvironment dfe) { //No need for custom parsing anymore! System.out.println(\"Rated \" + ratingInput.getTitle() + \" with \" + ratingInput.getStars() + \" stars\") ; System.out.println(\"DataFetchingEnvironment: \" + dfe.getArgument(ratingInput)); return new Rating(ratingInput.getStars()); } } Kotlin data types In Kotlin, you can use Data Classes to represent input types. However, make sure its fields are either var or add a @JsonProperty to each constructor argument, and use jacksonObjectMapper() to create a Kotlin-compatible Jackson mapper. data class RatingInput(var title: String, var stars: Int) --8<-- \"docs/reference_links\"","title":"Mutations"},{"location":"mutations/#input-types","text":"In the example above the input was two standard scalar types. You can also use complex types, and you should define these as input types in your schema. An input type is almost the same as a type in [GraphQL], but with some extra rules . According to the GraphQL specification an input type should always be passed to the data fetcher as a Map . This means the DataFetchingEnvironment.getArgument for an input type is a Map , and not the Java/Kotlin representation that you might have. The framework has a convenience mechanism around this, which will be discussed next. Let's first look at an example that uses DataFetchingEnvironment directly. type Mutation { addRating(input: RatingInput):Rating } input RatingInput { title: String, stars: Int } type Rating { avgStars: Float } @DgsComponent public class RatingMutation { @DgsData(parentType = \"Mutation\", field = \"addRating\") public Rating addRating(DataFetchingEnvironment dataFetchingEnvironment) { Map<String,Object> input = dataFetchingEnvironment.getArgument(\"input\"); RatingInput ratingInput = new ObjectMapper().convertValue(input, RatingInput.class); System.out.println(\"Rated \" + ratingInput.getTitle() + \" with \" + ratingInput.getStars() + \" stars\") ; return new Rating(ratingInput.getStars()); } } class RatingInput { private String title; private int stars; public String getTitle() { return title; } public void setTitle(String title) { this.title = title; } public int getStars() { return stars; } public void setStars(int stars) { this.stars = stars; } }","title":"Input Types"},{"location":"mutations/#input-arguments-as-data-fetchter-method-parameters","text":"The framework makes it easier to get input arguments. You can specify arguments as method parameters of a data fetcher. @DgsComponent public class RatingMutation { @DgsData(parentType = \"Mutation\", field = \"addRating\") public Rating addRating(@InputArgument(\"input\") RatingInput ratingInput) { //No need for custom parsing anymore! System.out.println(\"Rated \" + ratingInput.getTitle() + \" with \" + ratingInput.getStars() + \" stars\") ; return new Rating(ratingInput.getStars()); } } The @InputArgument annotation is important to specify the name of the input argument, because arguments can be specified in any order. If no annotation is present, the framework tries to use the parameter name, but this is only possible if the code is compiled with specific compiler settings . Input type parameters can be combined with a DataFetchingEnvironment parameter. @DgsComponent public class RatingMutation { @DgsData(parentType = \"Mutation\", field = \"addRating\") public Rating addRating(@InputArgument(\"input\") RatingInput ratingInput, DataFetchingEnvironment dfe) { //No need for custom parsing anymore! System.out.println(\"Rated \" + ratingInput.getTitle() + \" with \" + ratingInput.getStars() + \" stars\") ; System.out.println(\"DataFetchingEnvironment: \" + dfe.getArgument(ratingInput)); return new Rating(ratingInput.getStars()); } }","title":"Input arguments as data fetchter method parameters"},{"location":"mutations/#kotlin-data-types","text":"In Kotlin, you can use Data Classes to represent input types. However, make sure its fields are either var or add a @JsonProperty to each constructor argument, and use jacksonObjectMapper() to create a Kotlin-compatible Jackson mapper. data class RatingInput(var title: String, var stars: Int) --8<-- \"docs/reference_links\"","title":"Kotlin data types"},{"location":"query-execution-testing/","text":"The DGS framework allows you to write lightweight tests that partially bootstrap the framework, just enough to run queries. Example Create some tests for the hello query that you created in the Tutorial . Before writing tests, you need to add JUnit to the [Gradle] configuration. This example uses JUnit 5: testImplementation(\"org.junit.jupiter:junit-jupiter-api:5.5.1\") testRuntimeOnly(\"org.junit.jupiter:junit-jupiter-engine:5.5.1\") testCompile 'com.netflix.spring:spring-boot-netflix-starter-test' Create a test class with the following contents: import com.netflix.graphql.dgs.DgsQueryExecutor; import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import static org.assertj.core.api.Assertions.assertThat; @SpringBootTest(classes = {HelloDataFetcher.class, DgsAutoConfiguration.class}, // The following line disables DGS reload behavior in tests properties = {\"dgs.reload=false\"}) class HelloDataFetcherTest { @Autowired DgsQueryExecutor queryExecutor; @Test void helloShouldIncludeName() { String message = queryExecutor.executeAndExtractJsonPath(\"{hello(name: \\\"DGS\\\")}\", \"data.hello\"); assertThat(message).isEqualTo(\"hello, DGS!\"); } @Test void helloShouldWorkWithoutName() { String message = queryExecutor.executeAndExtractJsonPath(\"{hello}\", \"data.hello\"); assertThat(message).isEqualTo(\"hello, stranger!\"); } @Test void helloShouldIncludeNameWithVariables() { String message = queryExecutor.executeAndExtractJsonPath(\"query Hello($name: String) { hello(name: $name)}\", \"data.hello\", Maps.newHashMap(\"name\", \"DGS\")); assertThat(message).isEqualTo(\"hello, DGS!\"); } } The @SpringBootTest annotation makes this a Spring test. If you do not specify classes explicitly, Spring will start all components on the classpath. This includes not only your code, but also the full Netflix ecosystem. That really slows down tests, and most components aren\u2019t necessary for testing! It\u2019s much better to explicitly list the classes that you need for this test. In this case that\u2019s just HelloDataFetcher.class and DgsAutoConfiguration.class . HelloDataFetcher is obviously your code, and DgsAutoConfiguration starts the DGS-related components. To run queries, inject DgsQueryExecutor in the test. This interface has several methods to execute a query and get back the result. It executes the exact same code as a query on the /graphql endpoint would, but you won\u2019t have to deal with HTTP in your tests. The DgsQueryExecutor methods accept JSON paths, so that the methods can easily extract just the data from the response that you\u2019re interested in. The JSON paths are supported by the open source JsonPath library . Run the tests from your IDE, and you\u2019ll find that one of them is failing, because an empty name is not handled very nicely. That should be an easy fix, and demonstrates how easy it is to write useful tests. Building GraphQL Queries for Tests In the examples shown previously, we handcrafted the query string. This is simple enough for queries that are straightforward. However, constructing longer query strings can be tedious. For this, we can use the GraphQLQueryRequest to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . You will also need to add com.netflix.graphql.dgs:graphql-dgs-client:latest.release dependency to build.gradle. Now we can write a test that uses GraphQLQueryRequest to build the query and extract the response using GraphQLResponse for data fetchers for the following schema: type Query @extends { movieScript(movieId: ID): Movie } type Movie @key(fields: \"movieId\") @extends { movieId: ID! script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } The code generation plugin will generate the required POJOs, in addition to MovieScriptGraphQLQuery and a MovieScriptProjection . The MovieScriptGraphQLQuery has a builder to represent the query with input types. The MovieScriptProjection lets you specify the fields you want in the response. You can now set up your test like this: @Test void scriptShouldIncludeTitle() throws IOException { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new MovieScriptGraphQLQuery.Builder() .movieId(\"111888999\") .build(), new MovieScriptProjectionRoot().script().title().actors().name().age().parent().director() ); // This generates \"query {movieScript(movieId: \"111888999\"){ script { title actors { name age } director } } }\" String query = graphQLQueryRequest.serialize(); DocumentContext context = queryExecutor.executeAndGetDocumentContext(query); GraphQLResponse response = new GraphQLResponse(context.jsonString()); Movie movie = response.extractValueAsObject(\"data.movieScript\", Movie.class); assertThat(movie.getScript().getTitle()).isEqualTo(\"Top Secret\"); } The GraphQLQueryRequest is available as part of the graphql-client module and is used to build the query string, and wrap the response respectively. You can also refer to the GraphQLClient JavaDoc for more details on the list of supported methods. Mocking External Service Calls in Tests It\u2019s not uncommon for a data fetcher to talk to external systems: either a database or a remote service. If it does so within a test, this adds two problems: It adds latency; your tests are going to run slower when they make a lot of external calls. It adds flakiness: Did your code introduce a bug, or did something go wrong in the external system? In many cases it\u2019s better to mock these external services. Spring already has good support for doing so with the @Mockbean annotation, which you can leverage in your [DGS] tests. Example Introduce a fictional service client GreeterService to the project (in the real world this could be a [gRPC] client): public interface GreeterService { String randomGreeting(String name); } @Component public class FakeGreeterServiceImpl implements GreeterService { @Override public String randomGreeting(String name) { throw new RuntimeException(\"This is not a real client!\"); } } The hello data fetcher should use this new GreeterService : @DgsComponent public class HelloDataFetcher { @Autowired GreeterService greeterService; @DgsData(parentType = \"Query\", key = \"hello\") public String hello(DataFetchingEnvironment dfe) { String name = dfe.getArgument(\"name\"); return greeterService.randomGreeting(name); } } All tests now fail, since the FakeGreeterServiceImpl isn\u2019t included in the classes definition of @SpringBootTest . If you include it, the tests fail with the RuntimeException that\u2019s thrown from the randomGreeting() method. To add a mock to make the tests work again, add the following code to the test: @MockBean GreeterService greeterService; @Before void setup() { when(greeterService.randomGreeting(anyString())).thenAnswer(invocation -> \"Mocked greeting, \" + invocation.getArgument(0)); } Testing Exceptions The tests you wrote so far are mostly happy paths. Failure scenarios are also easy to test. Try a query with two fields that both return an error: @Test void getQueryWithMultipleExceptions() { try { queryExecutor.executeAndExtractJsonPath(\"{withRuntimeException, withGraphqlException}\", \"data.greeting\"); fail(\"Exception should have been thrown\"); } catch(QueryException ex) { assertThat(ex.getErrors().get(0).getMessage()).isEqualTo(\"java.lang.RuntimeException: That's broken!\"); assertThat(ex.getErrors().get(1).getMessage()).isEqualTo(\"graphql.GraphQLException: that's not going to work!\"); assertThat(ex.getMessage()).isEqualTo(\"java.lang.RuntimeException: That's broken!, graphql.GraphQLException: that's not going to work!\"); assertThat(ex.getErrors().size()).isEqualTo(2); } } When an error happens while executing the query, the errors are wrapped and thrown in a QueryException . This allows you to easily inspect the error. The message of the QueryException is the concatenation of all the errors. The getErrors() method gives access to the individual errors for further inspection. --8<-- \"docs/reference_links\"","title":"Testing"},{"location":"query-execution-testing/#example","text":"Create some tests for the hello query that you created in the Tutorial . Before writing tests, you need to add JUnit to the [Gradle] configuration. This example uses JUnit 5: testImplementation(\"org.junit.jupiter:junit-jupiter-api:5.5.1\") testRuntimeOnly(\"org.junit.jupiter:junit-jupiter-engine:5.5.1\") testCompile 'com.netflix.spring:spring-boot-netflix-starter-test' Create a test class with the following contents: import com.netflix.graphql.dgs.DgsQueryExecutor; import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import static org.assertj.core.api.Assertions.assertThat; @SpringBootTest(classes = {HelloDataFetcher.class, DgsAutoConfiguration.class}, // The following line disables DGS reload behavior in tests properties = {\"dgs.reload=false\"}) class HelloDataFetcherTest { @Autowired DgsQueryExecutor queryExecutor; @Test void helloShouldIncludeName() { String message = queryExecutor.executeAndExtractJsonPath(\"{hello(name: \\\"DGS\\\")}\", \"data.hello\"); assertThat(message).isEqualTo(\"hello, DGS!\"); } @Test void helloShouldWorkWithoutName() { String message = queryExecutor.executeAndExtractJsonPath(\"{hello}\", \"data.hello\"); assertThat(message).isEqualTo(\"hello, stranger!\"); } @Test void helloShouldIncludeNameWithVariables() { String message = queryExecutor.executeAndExtractJsonPath(\"query Hello($name: String) { hello(name: $name)}\", \"data.hello\", Maps.newHashMap(\"name\", \"DGS\")); assertThat(message).isEqualTo(\"hello, DGS!\"); } } The @SpringBootTest annotation makes this a Spring test. If you do not specify classes explicitly, Spring will start all components on the classpath. This includes not only your code, but also the full Netflix ecosystem. That really slows down tests, and most components aren\u2019t necessary for testing! It\u2019s much better to explicitly list the classes that you need for this test. In this case that\u2019s just HelloDataFetcher.class and DgsAutoConfiguration.class . HelloDataFetcher is obviously your code, and DgsAutoConfiguration starts the DGS-related components. To run queries, inject DgsQueryExecutor in the test. This interface has several methods to execute a query and get back the result. It executes the exact same code as a query on the /graphql endpoint would, but you won\u2019t have to deal with HTTP in your tests. The DgsQueryExecutor methods accept JSON paths, so that the methods can easily extract just the data from the response that you\u2019re interested in. The JSON paths are supported by the open source JsonPath library . Run the tests from your IDE, and you\u2019ll find that one of them is failing, because an empty name is not handled very nicely. That should be an easy fix, and demonstrates how easy it is to write useful tests.","title":"Example"},{"location":"query-execution-testing/#building-graphql-queries-for-tests","text":"In the examples shown previously, we handcrafted the query string. This is simple enough for queries that are straightforward. However, constructing longer query strings can be tedious. For this, we can use the GraphQLQueryRequest to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . You will also need to add com.netflix.graphql.dgs:graphql-dgs-client:latest.release dependency to build.gradle. Now we can write a test that uses GraphQLQueryRequest to build the query and extract the response using GraphQLResponse for data fetchers for the following schema: type Query @extends { movieScript(movieId: ID): Movie } type Movie @key(fields: \"movieId\") @extends { movieId: ID! script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } The code generation plugin will generate the required POJOs, in addition to MovieScriptGraphQLQuery and a MovieScriptProjection . The MovieScriptGraphQLQuery has a builder to represent the query with input types. The MovieScriptProjection lets you specify the fields you want in the response. You can now set up your test like this: @Test void scriptShouldIncludeTitle() throws IOException { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new MovieScriptGraphQLQuery.Builder() .movieId(\"111888999\") .build(), new MovieScriptProjectionRoot().script().title().actors().name().age().parent().director() ); // This generates \"query {movieScript(movieId: \"111888999\"){ script { title actors { name age } director } } }\" String query = graphQLQueryRequest.serialize(); DocumentContext context = queryExecutor.executeAndGetDocumentContext(query); GraphQLResponse response = new GraphQLResponse(context.jsonString()); Movie movie = response.extractValueAsObject(\"data.movieScript\", Movie.class); assertThat(movie.getScript().getTitle()).isEqualTo(\"Top Secret\"); } The GraphQLQueryRequest is available as part of the graphql-client module and is used to build the query string, and wrap the response respectively. You can also refer to the GraphQLClient JavaDoc for more details on the list of supported methods.","title":"Building GraphQL Queries for Tests"},{"location":"query-execution-testing/#mocking-external-service-calls-in-tests","text":"It\u2019s not uncommon for a data fetcher to talk to external systems: either a database or a remote service. If it does so within a test, this adds two problems: It adds latency; your tests are going to run slower when they make a lot of external calls. It adds flakiness: Did your code introduce a bug, or did something go wrong in the external system? In many cases it\u2019s better to mock these external services. Spring already has good support for doing so with the @Mockbean annotation, which you can leverage in your [DGS] tests.","title":"Mocking External Service Calls in Tests"},{"location":"query-execution-testing/#example_1","text":"Introduce a fictional service client GreeterService to the project (in the real world this could be a [gRPC] client): public interface GreeterService { String randomGreeting(String name); } @Component public class FakeGreeterServiceImpl implements GreeterService { @Override public String randomGreeting(String name) { throw new RuntimeException(\"This is not a real client!\"); } } The hello data fetcher should use this new GreeterService : @DgsComponent public class HelloDataFetcher { @Autowired GreeterService greeterService; @DgsData(parentType = \"Query\", key = \"hello\") public String hello(DataFetchingEnvironment dfe) { String name = dfe.getArgument(\"name\"); return greeterService.randomGreeting(name); } } All tests now fail, since the FakeGreeterServiceImpl isn\u2019t included in the classes definition of @SpringBootTest . If you include it, the tests fail with the RuntimeException that\u2019s thrown from the randomGreeting() method. To add a mock to make the tests work again, add the following code to the test: @MockBean GreeterService greeterService; @Before void setup() { when(greeterService.randomGreeting(anyString())).thenAnswer(invocation -> \"Mocked greeting, \" + invocation.getArgument(0)); }","title":"Example"},{"location":"query-execution-testing/#testing-exceptions","text":"The tests you wrote so far are mostly happy paths. Failure scenarios are also easy to test. Try a query with two fields that both return an error: @Test void getQueryWithMultipleExceptions() { try { queryExecutor.executeAndExtractJsonPath(\"{withRuntimeException, withGraphqlException}\", \"data.greeting\"); fail(\"Exception should have been thrown\"); } catch(QueryException ex) { assertThat(ex.getErrors().get(0).getMessage()).isEqualTo(\"java.lang.RuntimeException: That's broken!\"); assertThat(ex.getErrors().get(1).getMessage()).isEqualTo(\"graphql.GraphQLException: that's not going to work!\"); assertThat(ex.getMessage()).isEqualTo(\"java.lang.RuntimeException: That's broken!, graphql.GraphQLException: that's not going to work!\"); assertThat(ex.getErrors().size()).isEqualTo(2); } } When an error happens while executing the query, the errors are wrapped and thrown in a QueryException . This allows you to easily inspect the error. The message of the QueryException is the concatenation of all the errors. The getErrors() method gives access to the individual errors for further inspection. --8<-- \"docs/reference_links\"","title":"Testing Exceptions"},{"location":"scalars/","text":"It is easy to add a custom scalar type in the [DGS] framework: Create a class that implements the graphql.schema.Coercing interface and annotate it with the @DgsScalar annotation. Also make sure the scalar type is defined in your [GraphQL] schema! For example, this is a simple LocalDateTime implementation: @DgsScalar(name=\"DateTime\") public class DateTimeScalar implements Coercing<LocalDateTime, String> { @Override public String serialize(Object dataFetcherResult) throws CoercingSerializeException { if (dataFetcherResult instanceof LocalDateTime) { return ((LocalDateTime) dataFetcherResult).format(DateTimeFormatter.ISO_DATE_TIME); } else { throw new CoercingSerializeException(\"Not a valid DateTime\"); } } @Override public LocalDateTime parseValue(Object input) throws CoercingParseValueException { return LocalDateTime.parse(input.toString(), DateTimeFormatter.ISO_DATE_TIME); } @Override public LocalDateTime parseLiteral(Object input) throws CoercingParseLiteralException { if (input instanceof StringValue) { return LocalDateTime.parse(((StringValue) input).getValue(), DateTimeFormatter.ISO_DATE_TIME); } throw new CoercingParseLiteralException(\"Value is not a valid ISO date time\"); } } Schema: scalar DateTime Shared Scalars and Types The Studio Edge team is designing a set of common shared scalars and types for Studio Edge. See Schema Best Practices: Custom Scalars at the Studio Edge documentation for more details. --8<-- \"docs/reference_links\"","title":"Scalars"},{"location":"scalars/#shared-scalars-and-types","text":"The Studio Edge team is designing a set of common shared scalars and types for Studio Edge. See Schema Best Practices: Custom Scalars at the Studio Edge documentation for more details. --8<-- \"docs/reference_links\"","title":"Shared Scalars and Types"},{"location":"tutorial/","text":"In this tutorial you\u2019ll create a [GraphQL] domain graph service by using the [DGS] framework. You can run this service in a stand-alone fashion, and you can also plug it into the [Studio Edge Gateway]. The following is included in this tutorial: Adding the DGS framework dependency Creating a data fetcher Testing a data fetcher Extending the RuntimeWiring Adding type resolvers Adding [entity] fetchers and type resolvers for [federation] Overriding framework configuration and components - Creating a DataLoader Adding the [DGS] Framework Dependency This tutorial assumes that you have created a [Spring Boot] application by using Newt . To add DGS functionality, add a single dependency to build.gradle : dependencies { compile 'com.netflix.graphql.dgs:graphql-dgs-starter:latest.release' } This starter pulls in the set of dependencies required to use the DGS framework, and sets up AutoConfiguration that bootstraps the framework. You are now ready to design a schema and create the first data fetcher! Creating a Schema Before you worry about providing data, you should first design a schema for the data. You must add at least one schema file with the .graphqls extension in src/main/resources/schema/ . To get started, create a file src/main/resources/schema/schema.graphqls with the following contents. type Query { hello(name: String): String } This schema allows for a query like the following: query { hello(name:\"Paul\") } Such a query gives a result like the following: { \"data\": { \"hello\": \"hello, Paul!\" } } IDE Schema File Compilation If you are implementing a DGS, and therefore reference federation directives in your schema SDL (such as @extends ), you may see compile errors in your IDE. To fix this, add the federation directives to a directory whose schema files are not pushed to the schema registry. E.g., following the directions above, you can place your schema files (to push) in src/main/resources/schema/ , but the federation dependencies in a file like, src/main/resources/dontpush.graphqls and push only schema files found in the schema/ subdirectory. The schema file containing the shared federation directives and types will look something like this: # WARNING! DO NOT PUSH THIS SCHEMA # It's just to reduce IntelliJ compile errors scalar _Any scalar _FieldSet scalar DateTime # a union of all types that use the @key directive union _Entity type _Service { sdl: String } type Query @extends { _entities(representations: [_Any!]!): [_Entity]! _service: _Service! } directive @external on FIELD_DEFINITION directive @requires(fields: _FieldSet!) on FIELD_DEFINITION directive @provides(fields: _FieldSet!) on FIELD_DEFINITION directive @key(fields: _FieldSet!) on OBJECT | INTERFACE # this is an optional directive discussed below directive @extends on OBJECT | INTERFACE Now it's time to write some code! Creating a Data Fetcher A data fetcher is the method that returns the data for a specific key in a [GraphQL] query. The [DGS] framework has custom annotations that make it trivial to register data fetchers. Create a new class with the following contents: import com.netflix.graphql.dgs.DgsComponent; import com.netflix.graphql.dgs.DgsData; import graphql.schema.DataFetchingEnvironment; @DgsComponent public class HelloDataFetcher { @DgsData(parentType = \"Query\", field = \"hello\") public String hello(DataFetchingEnvironment dfe) { String name = dfe.getArgument(\"name\"); return \"hello, \" + name + \"!\"; } } The @DgsComponent annotation marks this class as a class that the DGS framework should be interested in. The actual capabilities of the class are defined by annotations on the method level, in this case @DgsData . The @DgsData annotation takes a parentType and key as arguments. The parentType is the object type, as defined in the GraphQL schema, on which a field is defined. The key is the name of the field. In the example schema, the hello field is defined on the Query object type. This means the parentType is Query and the field is hello for the @DsgData annotation. Write a method that you have annotated with @DgsData so that it receives an argument of type DataFetchingEnvironment , which is a type coming from the graphql-java library. This type gives access to arguments, the parent type, data loaders, etc. That\u2019s it! You\u2019re ready to accept GraphQL queries. The starter includes GraphiQL, which is available at https://localhost:8443/graphiql . You can enter the sample query from above and hit the play button to run your first DGS query: query { hello(name:\"Paul\") } !!!note \"Fun Fact\" The graphiql and graphql endpoints already use [Meechum] out of the box. Optimizing the Development Workflow [Spring Boot] with all its Netflix integrations is great for developers, except for the relatively slow application startups (1+ minute for even the simplest app). The slow startups are caused by the many integrations that are provided out-of-the-box to Netflix infrastructure. These integrations make using our infrastructure easy, but come at a cost when starting the application. Especially in the early stages of developing a [GraphQL] service, it\u2019s often helpful for you to make small experimental changes to both the schema and the datafetchers. Unfortunately, that workflow typically would require that you restart after each change, which obviously really slows down the development workflow. To some extent you can avoid this by using the framework\u2019s testing capabilities , but that\u2019s not always the easiest option. The [DGS] framework is optimized to prevent restarts during development. To get the most benefits (and pretty much eliminate restarts), install the commercial plugin JRebel . Although JRebel is commercial, and not cheap, it can drastically improve developer experience. !!!tip \"How to get JRebel\" Reach out to [#engineering-help] to see if any corporate JRebel licenses are available. Last time we checked, no such licenses were available through Netflix, and you were encouraged to purchase a personal license and expense it . Also, contact Jose Ferrel in Technology Deployment to let him know that there is interest for Netflix-wide JRebel licensing. JRebel hot-loads code changes and integrates with Spring Boot to dynamically reload Spring components whenever you make changes to them. The DGS framework picks up schema changes dynamically, and rewires DGS components such as data loaders and data fetchers. !!!tip \"Disabling Auto-Rewiring\" If you want to disable this automatic rewiring, add the following section to your application.yml : dgs: reload: false You could instead disable automatic rewiring for tests only by setting this property in the `properties` block of your `@SpringBootTest` declaration in the file where you import `DgsAutoConfiguration`: @SpringBootTest(classes= {some.class, another.class}, properties=\"dgs.reload=false\") See [Writing Tests](../testing/testing.md). Without JRebel, some of the functionality is still available. Schema changes are still dynamic, and basic code changes such changing the implementation of a method can be hot swapped by IntelliJ. However, if you make any structural changes to the code, or add new components, this will still require that you restart. --8<-- \"docs/reference_links\"","title":"Getting Started"},{"location":"tutorial/#adding-the-dgs-framework-dependency","text":"This tutorial assumes that you have created a [Spring Boot] application by using Newt . To add DGS functionality, add a single dependency to build.gradle : dependencies { compile 'com.netflix.graphql.dgs:graphql-dgs-starter:latest.release' } This starter pulls in the set of dependencies required to use the DGS framework, and sets up AutoConfiguration that bootstraps the framework. You are now ready to design a schema and create the first data fetcher!","title":"Adding the [DGS] Framework Dependency"},{"location":"tutorial/#creating-a-schema","text":"Before you worry about providing data, you should first design a schema for the data. You must add at least one schema file with the .graphqls extension in src/main/resources/schema/ . To get started, create a file src/main/resources/schema/schema.graphqls with the following contents. type Query { hello(name: String): String } This schema allows for a query like the following: query { hello(name:\"Paul\") } Such a query gives a result like the following: { \"data\": { \"hello\": \"hello, Paul!\" } }","title":"Creating a Schema"},{"location":"tutorial/#ide-schema-file-compilation","text":"If you are implementing a DGS, and therefore reference federation directives in your schema SDL (such as @extends ), you may see compile errors in your IDE. To fix this, add the federation directives to a directory whose schema files are not pushed to the schema registry. E.g., following the directions above, you can place your schema files (to push) in src/main/resources/schema/ , but the federation dependencies in a file like, src/main/resources/dontpush.graphqls and push only schema files found in the schema/ subdirectory. The schema file containing the shared federation directives and types will look something like this: # WARNING! DO NOT PUSH THIS SCHEMA # It's just to reduce IntelliJ compile errors scalar _Any scalar _FieldSet scalar DateTime # a union of all types that use the @key directive union _Entity type _Service { sdl: String } type Query @extends { _entities(representations: [_Any!]!): [_Entity]! _service: _Service! } directive @external on FIELD_DEFINITION directive @requires(fields: _FieldSet!) on FIELD_DEFINITION directive @provides(fields: _FieldSet!) on FIELD_DEFINITION directive @key(fields: _FieldSet!) on OBJECT | INTERFACE # this is an optional directive discussed below directive @extends on OBJECT | INTERFACE Now it's time to write some code!","title":"IDE Schema File Compilation"},{"location":"tutorial/#creating-a-data-fetcher","text":"A data fetcher is the method that returns the data for a specific key in a [GraphQL] query. The [DGS] framework has custom annotations that make it trivial to register data fetchers. Create a new class with the following contents: import com.netflix.graphql.dgs.DgsComponent; import com.netflix.graphql.dgs.DgsData; import graphql.schema.DataFetchingEnvironment; @DgsComponent public class HelloDataFetcher { @DgsData(parentType = \"Query\", field = \"hello\") public String hello(DataFetchingEnvironment dfe) { String name = dfe.getArgument(\"name\"); return \"hello, \" + name + \"!\"; } } The @DgsComponent annotation marks this class as a class that the DGS framework should be interested in. The actual capabilities of the class are defined by annotations on the method level, in this case @DgsData . The @DgsData annotation takes a parentType and key as arguments. The parentType is the object type, as defined in the GraphQL schema, on which a field is defined. The key is the name of the field. In the example schema, the hello field is defined on the Query object type. This means the parentType is Query and the field is hello for the @DsgData annotation. Write a method that you have annotated with @DgsData so that it receives an argument of type DataFetchingEnvironment , which is a type coming from the graphql-java library. This type gives access to arguments, the parent type, data loaders, etc. That\u2019s it! You\u2019re ready to accept GraphQL queries. The starter includes GraphiQL, which is available at https://localhost:8443/graphiql . You can enter the sample query from above and hit the play button to run your first DGS query: query { hello(name:\"Paul\") } !!!note \"Fun Fact\" The graphiql and graphql endpoints already use [Meechum] out of the box.","title":"Creating a Data Fetcher"},{"location":"tutorial/#optimizing-the-development-workflow","text":"[Spring Boot] with all its Netflix integrations is great for developers, except for the relatively slow application startups (1+ minute for even the simplest app). The slow startups are caused by the many integrations that are provided out-of-the-box to Netflix infrastructure. These integrations make using our infrastructure easy, but come at a cost when starting the application. Especially in the early stages of developing a [GraphQL] service, it\u2019s often helpful for you to make small experimental changes to both the schema and the datafetchers. Unfortunately, that workflow typically would require that you restart after each change, which obviously really slows down the development workflow. To some extent you can avoid this by using the framework\u2019s testing capabilities , but that\u2019s not always the easiest option. The [DGS] framework is optimized to prevent restarts during development. To get the most benefits (and pretty much eliminate restarts), install the commercial plugin JRebel . Although JRebel is commercial, and not cheap, it can drastically improve developer experience. !!!tip \"How to get JRebel\" Reach out to [#engineering-help] to see if any corporate JRebel licenses are available. Last time we checked, no such licenses were available through Netflix, and you were encouraged to purchase a personal license and expense it . Also, contact Jose Ferrel in Technology Deployment to let him know that there is interest for Netflix-wide JRebel licensing. JRebel hot-loads code changes and integrates with Spring Boot to dynamically reload Spring components whenever you make changes to them. The DGS framework picks up schema changes dynamically, and rewires DGS components such as data loaders and data fetchers. !!!tip \"Disabling Auto-Rewiring\" If you want to disable this automatic rewiring, add the following section to your application.yml : dgs: reload: false You could instead disable automatic rewiring for tests only by setting this property in the `properties` block of your `@SpringBootTest` declaration in the file where you import `DgsAutoConfiguration`: @SpringBootTest(classes= {some.class, another.class}, properties=\"dgs.reload=false\") See [Writing Tests](../testing/testing.md). Without JRebel, some of the functionality is still available. Schema changes are still dynamic, and basic code changes such changing the implementation of a method can be hot swapped by IntelliJ. However, if you make any structural changes to the code, or add new components, this will still require that you restart. --8<-- \"docs/reference_links\"","title":"Optimizing the Development Workflow"},{"location":"advanced/custom-datafetcher-context/","text":"Each data fetcher in [GraphQL] Java has a context. A data fetcher gets access to its context by calling DataFetchingEnvironment.getContext() . This is a common mechanism to pass request context to data fetchers and data loaders. The [DGS] framework has its own DgsContext implementation, which is used for log instrumentation among other things. It is designed in such a way that you can extend it with your own custom context. To create a custom context, implement a Spring bean of type DgsCustomContextBuilder . Write the build() method so that it creates an instance of the type that represents your custom context object: @Component public class MyContextBuilder implements DgsCustomContextBuilder<MyContext> { @Override public MyContext build() { return new MyContext(); } } public class MyContext { private final String customState = \"Custom state!\"; public String getCustomState() { return customState; } } A data fetcher can now retrieve the context by calling the getCustomContext() method: @DgsData(parentType = \"Query\", field = \"withContext\") public String withContext(DataFetchingEnvironment dfe) { MyContext customContext = DgsContext.getCustomContext(dfe); return customContext.getCustomState(); } Similarly it can be used in a DataLoader. @DgsDataLoader(name = \"exampleLoaderWithContext\") public class ExampleLoaderWithContext implements BatchLoaderWithContext<String, String> { @Override public CompletionStage<List<String>> load(List<String> keys, BatchLoaderEnvironment environment) { MyContext context = DgsContext.getCustomContext(environment); return CompletableFuture.supplyAsync(() -> keys.stream().map(key -> context.getCustomState() + \" \" + key).collect(Collectors.toList())); } } --8<-- \"docs/reference_links\"","title":"Data Fetching Context"},{"location":"advanced/customizing/","text":"The [DGS] framework provides a [paved path] for common [GraphQL] use cases. For most use cases the framework provides annotations to register things such as data fetchers and data loaders. Besides easing use, the annotation-driven approach also allows us to plug in features such as metrics and tracing without the need for extra code. It is highly recommended that you follow this paved path; if there are any additional requirements, discuss them on [#studio-edge-devex]. However, you can customize the framework in case there are requirements not (yet) supported with higher level concepts. The framework integrates with [Spring Boot], and follows the auto-configuration mechanism that Spring uses. This allows a developer to plug in certain aspects to the framework, or to completely replace components provided by the framework. The DgsAutoConfiguration class is responsible for auto configuration. The framework defines each component with the following annotations: @Bean @ConditionalOnMissingBean This means that if you provide your own bean of the same type, the framework will use your instance instead of creating the default one. This way, you can customize any of the components created by the framework. If you find reasons to customize the framework, please let the Studio Edge Developer Experience team know at [#studio-edge-devex], so that they can help you to configure things correctly, and also explore if better alternatives can be provided by the framework. --8<-- \"docs/reference_links\"","title":"Custom Extensions"},{"location":"advanced/cyclical-queries/","text":"If you have cyclical queries in your graph, you may want to limit the possible query depth. You can use instrumentation to monitor and restrict query depth. In graphql-java , you can find an implementation for this in MaxQueryDepthInstrumentation.java . If you register an Instrumentation class as a Spring bean, the [DGS] framework will pick it up automatically. For example: @Bean public Instrumentation maxQueryDepthInstrumentation() { return new MaxQueryDepthInstrumentation(5); } --8<-- \"docs/reference_links\"","title":"Cyclical queries"},{"location":"advanced/error-handling/","text":"It is common in GraphQL to support error reporting by adding an errors block to a response. Responses can contain both data and errors, for example when some fields where resolved successfully, but other fields had errors. A field with an error is set to null, and an error is added to the errors block. An error typically contains the following fields: field type description message (non-nullable) String! a string description of the error intended for the developer as a guide to understand and correct the error locations [Location] an array of code locations, where each location is a map with the keys line and column , both natural numbers starting from 1 that describe the beginning of an associated syntax element path [String | Int] if the error is associated with one or more particular fields in the response, this field of the error details the paths of those response fields that experienced the error (this allows clients to identify whether a null result is intentional or caused by a runtime error) extensions [TypedError] see \u201cThe TypedError Interface\u201d below At Netflix we have a specification available for GraphQL errors. This specification lists types of errors, and specifies how these errors map to certain GraphQL errors. The specification can be found here . The DGS framework has an exception handler out-of-the-box that works according to the specification. This exception handler handles exceptions from data fetchers. Any RuntimeException is translated to a GraphQLError of type INTERNAL . For some specific exception types, a more specific GraphQL error type is used. Exception type GraphQL error type description AccessDeniedException PERMISSION_DENIED When a @Secured check fails DgsEntityNotFoundException NOT_FOUND Thrown by the developer when a requested entity (e.g. based on query parameters) isn't found Mapping custom exceptions It can be useful to map application specific exceptions to meaningful exceptions back to the client. You can do this by registering a DataFetcherExceptionHandler . Make sure to either extend or delegate to the DefaultDataFetcherExceptionHandler class, this is the default exception handler of the framework. If you don't extend/delegate to this class, you lose the framework's built-in exception handler. The following is an example of a custom exception handler implementation. @Component public class CustomDataFetchingExceptionHandler implements DataFetcherExceptionHandler { private final DefaultDataFetcherExceptionHandler defaultHandler = new DefaultDataFetcherExceptionHandler(); @Override public DataFetcherExceptionHandlerResult onException(DataFetcherExceptionHandlerParameters handlerParameters) { if(handlerParameters.getException() instanceof MyException) { Map<String, Object> debugInfo = new HashMap<>(); debugInfo.put(\"somefield\", \"somevalue\"); GraphQLError graphqlError = TypedGraphQLError.INTERNAL.message(\"This custom thing went wrong!\") .debugInfo(debugInfo) .path(handlerParameters.getPath()).build(); return DataFetcherExceptionHandlerResult.newResult() .error(graphqlError) .build(); } else { return defaultHandler.onException(handlerParameters); } } } The following data fetcher throws MyException . @DgsComponent public class HelloDataFetcher { @DgsData(parentType = \"Query\", field = \"hello\") @DgsEnableDataFetcherInstrumentation(false) public String hello(DataFetchingEnvironment dfe) { throw new MyException(); } } Querying the hello field results in the following response. { \"errors\": [ { \"message\": \"This custom thing went wrong!\", \"locations\": [], \"path\": [ \"hello\" ], \"extensions\": { \"errorType\": \"INTERNAL\", \"debugInfo\": { \"somefield\": \"somevalue\" } } } ], \"data\": { \"hello\": null } } !!!info Implementing your own DataFetcherExceptionHandler is also useful to add custom logging.","title":"Error Handling"},{"location":"advanced/error-handling/#mapping-custom-exceptions","text":"It can be useful to map application specific exceptions to meaningful exceptions back to the client. You can do this by registering a DataFetcherExceptionHandler . Make sure to either extend or delegate to the DefaultDataFetcherExceptionHandler class, this is the default exception handler of the framework. If you don't extend/delegate to this class, you lose the framework's built-in exception handler. The following is an example of a custom exception handler implementation. @Component public class CustomDataFetchingExceptionHandler implements DataFetcherExceptionHandler { private final DefaultDataFetcherExceptionHandler defaultHandler = new DefaultDataFetcherExceptionHandler(); @Override public DataFetcherExceptionHandlerResult onException(DataFetcherExceptionHandlerParameters handlerParameters) { if(handlerParameters.getException() instanceof MyException) { Map<String, Object> debugInfo = new HashMap<>(); debugInfo.put(\"somefield\", \"somevalue\"); GraphQLError graphqlError = TypedGraphQLError.INTERNAL.message(\"This custom thing went wrong!\") .debugInfo(debugInfo) .path(handlerParameters.getPath()).build(); return DataFetcherExceptionHandlerResult.newResult() .error(graphqlError) .build(); } else { return defaultHandler.onException(handlerParameters); } } } The following data fetcher throws MyException . @DgsComponent public class HelloDataFetcher { @DgsData(parentType = \"Query\", field = \"hello\") @DgsEnableDataFetcherInstrumentation(false) public String hello(DataFetchingEnvironment dfe) { throw new MyException(); } } Querying the hello field results in the following response. { \"errors\": [ { \"message\": \"This custom thing went wrong!\", \"locations\": [], \"path\": [ \"hello\" ], \"extensions\": { \"errorType\": \"INTERNAL\", \"debugInfo\": { \"somefield\": \"somevalue\" } } } ], \"data\": { \"hello\": null } } !!!info Implementing your own DataFetcherExceptionHandler is also useful to add custom logging.","title":"Mapping custom exceptions"},{"location":"advanced/file-uploads/","text":"Studio Edge has a variety of client applications that perform file uploads of scripts, images, and other resources through [DGS]es. In [GraphQL], you model a file upload operation as a GraphQL mutation request from a client to your DGS. The following sections describe how you implement file uploads and downloads. You use one of the following two approaches, depending on whether the file upload query goes directly to your DGS from a client, or it goes to your DGS through the [gateway]: multipart upload requests for file transfers direct to DGS (not through the gateway) pre-signed URLs generated by your DGS for file transfer queries that are part of the [federated] graph !!!tip For more context on file uploads and best practices, see Apollo Server File Upload Best Practices by Khalil Stemmler from Apollo Blog . Multipart File Upload A multipart request is an HTTP request that contains multiple parts in a single request: the mutation query, file data, JSON objects, and whatever else you like. You can use Apollo\u2019s upload client, or even a simple cURL, to send along a stream of file data using a multipart request that you model in your schema as a Mutation. !!!info See GraphQL multipart request specification for the specification of a multipart POST request for uploading files using GraphQL mutations. The DGS framework supports the Upload scalar with which you can specify files in your mutation query as a MultipartFile . When you send a multipart request for file upload, the framework processes each part and assembles the final GraphQL query that it hands to your data fetcher for further processing. Here is an example of a Mutation query that uploads a file to your DGS: \"\"\" For uploads using multi-part POST only. DO NOT include in federated scheme registered with gateway!!\"\"\" scalar Upload extend type Mutation { uploadScriptWithMultipartPOST(input: Upload!): Boolean } Note that you need to declare the Upload scalar in your schema, although the implementation is provided by the DGS framework. Also, if you register the graph that your DGS implements with the gateway, you must exclude the file upload parts of the schema (the Upload scalar and your Mutation query) from the schema that you publish to the [schema registry]. You can do this by separating those parts of the schema into a separate schema file and by not adding that file to your list of schema files to publish when you configure your schema validation plugin . In your DGS, add a data fetcher to handle this as a MultipartFile as shown here: @DgsData(parentType = DgsConstants.MUTATION.TYPE_NAME, field = \"uploadScriptWithMultipartPOST\") public boolean uploadScript(DataFetchingEnvironment dfe) throws IOException { // NOTE: Cannot use @InputArgument or Object Mapper to convert to class, because MultipartFile cannot be // deserialized MultipartFile file = dfe.getArgument(\"input\"); String content = new String(file.getBytes()); return ! content.isEmpty(); } Note that you will not be able to use a Jackson object mapper to deserialize a type that contains a MultipartFile , so you will need to explicitly get the file argument from your input. On your client, you can use apollo-upload-client to send your Mutation query as a multipart POST request with file data. Here\u2019s how you configure your link: import { createUploadLink } from 'apollo-upload-client' const uploadLink = createUploadLink({ uri: uri }) const authedClient = authLink && new ApolloClient({ link: uploadLink)), cache: new InMemoryCache() }) Once you set this up, set up your Mutation query and the pass the file that the user selected as a variable: // query for file uploads using multipart post const UploadScriptMultipartMutation_gql = gql` mutation uploadScriptWithMultipartPOST($input: Upload!) { uploadScriptWithMultipartPOST(input: $input) } `; function MultipartScriptUpload() { const [ uploadScriptMultipartMutation, { loading: mutationLoading, error: mutationError, data: mutationData, }, ] = useMutation(UploadScriptMultipartMutation_gql); const [scriptMultipartInput, setScriptMultipartInput] = useState<any>(); const onSubmitScriptMultipart = () => { const fileInput = scriptMultipartInput.files[0]; uploadScriptMultipartMutation({ variables: { input: fileInput }, }); }; return ( <div> <h3> Upload script using multipart HTTP POST</h3> <form onSubmit={e => { e.preventDefault(); onSubmitScriptMultipart(); }}> <label> <input type=\"file\" ref={ref => { setScriptMultipartInput(ref!); }} /> </label> <br /> <br /> <button type=\"submit\">Submit</button> </form> </div> ); } For a complete example, see ScriptUploadMultipartPOST.tsx from the file-uploads-example-ui project. !!!caution \"Important\" This mechanism is not supported by the Studio Edge Gateway and you should use it only for uploading files directly to the DGS from the client. To do this, you must configure your DGS so that your client can talk to it through [Wall-E]. Uploading and Downloading Files Using Presigned URLs When you implement file mutation queries that will be part of the federated graph, we recommend that you use pre-signed URLs. A pre-signed URL is an authenticated URL that your DGS can generate and return to the client, based on filename, sender, etc. The client uses this URL to upload files to, or download files from, the file storage service directly. This avoids streaming data through the gateway, and is the preferred method to avoid affecting the performance of the gateway. The pre-signed URL flow follows this sequence: The client sends to the DGS a Mutation query that specifies the file name. This query may or may not contain additional metadata to be handled by the DGS. The DGS authorizes the query based on the DGS [AuthZ] policies. It requests a signed token from a file storage service (e.g., Baggins) based on the requestor\u2019s email and the file name. The DGS responds to the client with the presigned URL (represented as a PresignedUrlResponse which is available as a common type). The client uploads or downloads files to the file storage service by using the URL in the response. The DGS can asynchronously monitor the status of the file upload operation. Once the upload is complete, the DGS can operate on additional metadata, and/or download the file for post-processing once the upload is complete. At this point, the client can send additional metadata if it did not do so already as part of the initial request. The client can also query the DGS to determine the success of the operation using a separate status update query specifying the uploadID , if one is returned by the DGS. To maintain consistency across clients, implement the following interface for the response containing pre-signed URLs: \"\"\" Representation of a response to a request for file upload or download containing a URL. \"\"\" interface PresignedUrlResponse { \"\"\" A pre-signed file URL for uploads and downloads generated using a backend service. \"\"\" url: String \"\"\" The custom headers that need to be set by the client in the upload/download request. \"\"\" headers: [Header] \"\"\" The method, e.g., POST, to use for uploading or downloading files using the URL. \"\"\" method: String } \"\"\" Representation of an HTTP request header with name of the header and the value. \"\"\" type Header { \"\"\" The name of the header. \"\"\" name: String \"\"\" The value of the header. \"\"\" value: String } This interface is available in common types library, so you only need to add an implementation in your application. The following sections describe the preferred way to use pre-signed URLs with a sample DGS and React application. For file storage, we recommend using Baggins , which is a Netflix-maintained file storage service that provides encryption on top of S3. !!!tip For more information on Baggins, see the Baggins Architecture slide deck. You can use Baggins for the paved-path experience, or you can choose any other file storage service such as Google Drive to suit your needs. !!!note None of these design choices are built in to the DGS framework, so you can implement a solution based on your application\u2019s requirements. Set up Baggins First, set up your keyspace (S3 bucket) where your files will be stored in Baggins for your application. You can contact the Baggins team ( #mce-baggins ) to help you with the setup. You will need a [Gandalf] policy that specifies access policies. We recommend that this policy be the same as your DGS Gandalf policy. The default DGS policy provides access to all Netflix employees. Next, configure your Baggins client. The example illustrated here uses the Baggins [gRPC] client to request for signed URLs. Add the following dependency to your build.gradle : // Baggins gRPC client compile 'com.netflix.baggins:baggins-client-grpc-spring:latest.release' compile 'com.netflix.spring:spring-boot-netflix-grpc-client-annotation:latest.release' You can use the gRPC client to request pre-signed URLs, as shown for example in BagginsService.java from the file-uploads-example-dgs project. Sample Schema Next, set up the schema as shown below by modeling file uploads as a Mutation query with some metadata. type Mutation @extends { uploadScriptWithPresignedUrl(input: UploadScriptRequestInput): PresignedUrlResponse } input UploadScriptRequestInput { name: String description: String filePath: String fileSize: Int } type UploadScriptResponse implements PresignedUrlResponse { url: String headers: [Header] method: String uploadId: ID } Set up a resolver You will also need to set up a type resolver for the PresignedUrlResponse interface that you have implemented. Here is an example: @DgsTypeResolver(name = \"PresignedUrlResponse\") public String resolvePresignedUrlResponse(PresignedUrlResponse response) { if (response instanceof UploadScriptResponse) { return \"UploadScriptResponse\"; } else { throw new RuntimeException(\"Invalid type: \" + response.getClass().getName() + \" found in PresignedUrlResponseResolver\"); } } DGS Data Fetcher Your data fetcher in your DGS handles the above Mutation query by generating or requesting a pre-signed URL from Baggins, as shown here: @DgsData(parentType = DgsConstants.MUTATION.TYPE_NAME, field = \"uploadScriptWithPresignedUrl\") public UploadScriptResponse uploadScript(@InputArgument(\"input\") UploadScriptRequestInput uploadRequest, DataFetchingEnvironment dataFetchingEnvironment) throws IOException { BagginsService.GetTokenResponse response = bagginsService.getSignedUploadUrl(uploadRequest.getFilePath()); CompletableFuture<Void> future = CompletableFuture.runAsync(() -> { long timeout = 600000; long startTime = System.currentTimeMillis(); long elapsed = startTime; boolean status = false; while (!status && elapsed - startTime < timeout) { try { TimeUnit.SECONDS.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } status = bagginsService.checkFileUploadStatus(response.keyname); elapsed = System.currentTimeMillis() - startTime; } if (elapsed - startTime >= timeout) { logger.error(\"Encountered a timeout waiting for file %s to upload\", response.keyname); } // You can perform your post upload actions at this point - e.g., download the file for post processing // or process meta data try { bagginsService.downloadFile(response.keyname); } catch (IOException e) { e.printStackTrace(); } }); Integer fileSize = uploadRequest.getFileSize(); PresignedUrlResponse.Header header1 = new PresignedUrlResponse.Header(); header1.setName(\"Content-Type\"); header1.setValue(\"application/octet-stream\"); PresignedUrlResponse.Header header2 = new PresignedUrlResponse.Header(); header2.setName(\"x-baggins-upload-file-length\"); header2.setValue(fileSize.toString()); List<PresignedUrlResponse.Header> headers = new ArrayList<>(Arrays.asList(header1, header2)); return new UploadScriptResponse( response.token, headers, \"POST\", response.keyname); } To ensure consistency from a client perspective for file uploads and downloads, we recommend the following: Set up a CompletableFuture to poll on the status of the file upload by the client. Upon completion, carry out any post-processing, such as updating metadata or downloading the file for validation, etc. The client can check the status of the mutation operation (including metadata updates, etc.) via a separate status query. The input arguments can contain the header-specific information, such as the file size in case of using Baggins. The client can set these headers without having to know about the details of the file storage service. From the client perspective it is a URL to upload the file to, and any additional information (such as the headers and HTTP method to use) are opaque to the client. Client Example In your client application, set up your Mutation query to request a pre-signed URL from the DGS. The client can use this URL to upload files directly to Baggins. After file uploads complete, the client can issue a subsequent query to the DGS if the client needs to check on the status of the mutation operation. For simple use cases, the client could just fire and forget. Such a second status query can be useful to the client in scenarios involving post-processing of the file by the DGS, such as validation of the file, or updating metadata in another service, e.g. Amsterdam . Here is an example client implementation showing the set up of queries in the UI application: // query for file uploads using pre-signed urls const PresignedUrlDetails_gql = gql` fragment PresignedUrlDetails on PresignedUrlResponse { url headers { name value } method } `; const UploadScriptRequestMutation_gql = gql` mutation uploadScriptWithPresignedUrl( $input: UploadScriptRequestInput ) { uploadScriptWithPresignedUrl(input: $input) { ...PresignedUrlDetails } } ${PresignedUrlDetails_gql} `; function ScriptUploadWithPresignedURL() { const [ scriptUpload, { loading: mutationLoading, error: mutationError, data: mutationData, }, ] = useMutation(UploadScriptRequestMutation_gql); const [scriptInput, setScriptInput] = useState<HTMLInputElement>(); type Header = { name: string; value: string; }; const uploadToUrl = ( url: string, headers: [Header], method: string, id: string, fileElement: HTMLInputElement ) => { if ( fileElement.type !== 'file' && fileElement.files!.length <= 0 ) { console.log('Incorrect type of file input'); return false; } const requestHeaders: HeadersInit = new Headers(); headers.forEach(iter => { requestHeaders.set(iter.name, iter.value); }); fetch(url, { method: method, headers: requestHeaders, body: fileElement.files![0], }) .then(result => { console.log(`Response from baggins ${result}`); }) .catch(error => { alert(`ERROR ${JSON.stringify(error)}`); console.log(`Received error ${JSON.stringify(error)}`); }); }; useEffect(() => { if (mutationData) { uploadToUrl( mutationData.uploadScriptWithPresignedUrl.url, mutationData.uploadScriptWithPresignedUrl.headers, mutationData.uploadScriptWithPresignedUrl.method, mutationData.uploadScriptWithPresignedUrl.uploadId, scriptInput! ); } }, [mutationData, scriptInput]); const onSubmitScriptPresigned = () => { const path = `${scriptInput!.files![0].name}`; const size = scriptInput!.files![0].size; scriptUpload({ variables: { input: { name: 'kavitha', description: 'script upload', filePath: path, fileSize: size, }, }, }); }; } The complete example is available in ScriptUploadWithPresignedURL.tsx from the file-uploads-example-ui project. --8<-- \"docs/reference_links\"","title":"File Uploads"},{"location":"advanced/file-uploads/#multipart-file-upload","text":"A multipart request is an HTTP request that contains multiple parts in a single request: the mutation query, file data, JSON objects, and whatever else you like. You can use Apollo\u2019s upload client, or even a simple cURL, to send along a stream of file data using a multipart request that you model in your schema as a Mutation. !!!info See GraphQL multipart request specification for the specification of a multipart POST request for uploading files using GraphQL mutations. The DGS framework supports the Upload scalar with which you can specify files in your mutation query as a MultipartFile . When you send a multipart request for file upload, the framework processes each part and assembles the final GraphQL query that it hands to your data fetcher for further processing. Here is an example of a Mutation query that uploads a file to your DGS: \"\"\" For uploads using multi-part POST only. DO NOT include in federated scheme registered with gateway!!\"\"\" scalar Upload extend type Mutation { uploadScriptWithMultipartPOST(input: Upload!): Boolean } Note that you need to declare the Upload scalar in your schema, although the implementation is provided by the DGS framework. Also, if you register the graph that your DGS implements with the gateway, you must exclude the file upload parts of the schema (the Upload scalar and your Mutation query) from the schema that you publish to the [schema registry]. You can do this by separating those parts of the schema into a separate schema file and by not adding that file to your list of schema files to publish when you configure your schema validation plugin . In your DGS, add a data fetcher to handle this as a MultipartFile as shown here: @DgsData(parentType = DgsConstants.MUTATION.TYPE_NAME, field = \"uploadScriptWithMultipartPOST\") public boolean uploadScript(DataFetchingEnvironment dfe) throws IOException { // NOTE: Cannot use @InputArgument or Object Mapper to convert to class, because MultipartFile cannot be // deserialized MultipartFile file = dfe.getArgument(\"input\"); String content = new String(file.getBytes()); return ! content.isEmpty(); } Note that you will not be able to use a Jackson object mapper to deserialize a type that contains a MultipartFile , so you will need to explicitly get the file argument from your input. On your client, you can use apollo-upload-client to send your Mutation query as a multipart POST request with file data. Here\u2019s how you configure your link: import { createUploadLink } from 'apollo-upload-client' const uploadLink = createUploadLink({ uri: uri }) const authedClient = authLink && new ApolloClient({ link: uploadLink)), cache: new InMemoryCache() }) Once you set this up, set up your Mutation query and the pass the file that the user selected as a variable: // query for file uploads using multipart post const UploadScriptMultipartMutation_gql = gql` mutation uploadScriptWithMultipartPOST($input: Upload!) { uploadScriptWithMultipartPOST(input: $input) } `; function MultipartScriptUpload() { const [ uploadScriptMultipartMutation, { loading: mutationLoading, error: mutationError, data: mutationData, }, ] = useMutation(UploadScriptMultipartMutation_gql); const [scriptMultipartInput, setScriptMultipartInput] = useState<any>(); const onSubmitScriptMultipart = () => { const fileInput = scriptMultipartInput.files[0]; uploadScriptMultipartMutation({ variables: { input: fileInput }, }); }; return ( <div> <h3> Upload script using multipart HTTP POST</h3> <form onSubmit={e => { e.preventDefault(); onSubmitScriptMultipart(); }}> <label> <input type=\"file\" ref={ref => { setScriptMultipartInput(ref!); }} /> </label> <br /> <br /> <button type=\"submit\">Submit</button> </form> </div> ); } For a complete example, see ScriptUploadMultipartPOST.tsx from the file-uploads-example-ui project. !!!caution \"Important\" This mechanism is not supported by the Studio Edge Gateway and you should use it only for uploading files directly to the DGS from the client. To do this, you must configure your DGS so that your client can talk to it through [Wall-E].","title":"Multipart File Upload"},{"location":"advanced/file-uploads/#uploading-and-downloading-files-using-presigned-urls","text":"When you implement file mutation queries that will be part of the federated graph, we recommend that you use pre-signed URLs. A pre-signed URL is an authenticated URL that your DGS can generate and return to the client, based on filename, sender, etc. The client uses this URL to upload files to, or download files from, the file storage service directly. This avoids streaming data through the gateway, and is the preferred method to avoid affecting the performance of the gateway. The pre-signed URL flow follows this sequence: The client sends to the DGS a Mutation query that specifies the file name. This query may or may not contain additional metadata to be handled by the DGS. The DGS authorizes the query based on the DGS [AuthZ] policies. It requests a signed token from a file storage service (e.g., Baggins) based on the requestor\u2019s email and the file name. The DGS responds to the client with the presigned URL (represented as a PresignedUrlResponse which is available as a common type). The client uploads or downloads files to the file storage service by using the URL in the response. The DGS can asynchronously monitor the status of the file upload operation. Once the upload is complete, the DGS can operate on additional metadata, and/or download the file for post-processing once the upload is complete. At this point, the client can send additional metadata if it did not do so already as part of the initial request. The client can also query the DGS to determine the success of the operation using a separate status update query specifying the uploadID , if one is returned by the DGS. To maintain consistency across clients, implement the following interface for the response containing pre-signed URLs: \"\"\" Representation of a response to a request for file upload or download containing a URL. \"\"\" interface PresignedUrlResponse { \"\"\" A pre-signed file URL for uploads and downloads generated using a backend service. \"\"\" url: String \"\"\" The custom headers that need to be set by the client in the upload/download request. \"\"\" headers: [Header] \"\"\" The method, e.g., POST, to use for uploading or downloading files using the URL. \"\"\" method: String } \"\"\" Representation of an HTTP request header with name of the header and the value. \"\"\" type Header { \"\"\" The name of the header. \"\"\" name: String \"\"\" The value of the header. \"\"\" value: String } This interface is available in common types library, so you only need to add an implementation in your application. The following sections describe the preferred way to use pre-signed URLs with a sample DGS and React application. For file storage, we recommend using Baggins , which is a Netflix-maintained file storage service that provides encryption on top of S3. !!!tip For more information on Baggins, see the Baggins Architecture slide deck. You can use Baggins for the paved-path experience, or you can choose any other file storage service such as Google Drive to suit your needs. !!!note None of these design choices are built in to the DGS framework, so you can implement a solution based on your application\u2019s requirements.","title":"Uploading and Downloading Files Using Presigned URLs"},{"location":"advanced/file-uploads/#set-up-baggins","text":"First, set up your keyspace (S3 bucket) where your files will be stored in Baggins for your application. You can contact the Baggins team ( #mce-baggins ) to help you with the setup. You will need a [Gandalf] policy that specifies access policies. We recommend that this policy be the same as your DGS Gandalf policy. The default DGS policy provides access to all Netflix employees. Next, configure your Baggins client. The example illustrated here uses the Baggins [gRPC] client to request for signed URLs. Add the following dependency to your build.gradle : // Baggins gRPC client compile 'com.netflix.baggins:baggins-client-grpc-spring:latest.release' compile 'com.netflix.spring:spring-boot-netflix-grpc-client-annotation:latest.release' You can use the gRPC client to request pre-signed URLs, as shown for example in BagginsService.java from the file-uploads-example-dgs project.","title":"Set up Baggins"},{"location":"advanced/file-uploads/#sample-schema","text":"Next, set up the schema as shown below by modeling file uploads as a Mutation query with some metadata. type Mutation @extends { uploadScriptWithPresignedUrl(input: UploadScriptRequestInput): PresignedUrlResponse } input UploadScriptRequestInput { name: String description: String filePath: String fileSize: Int } type UploadScriptResponse implements PresignedUrlResponse { url: String headers: [Header] method: String uploadId: ID }","title":"Sample Schema"},{"location":"advanced/file-uploads/#set-up-a-resolver","text":"You will also need to set up a type resolver for the PresignedUrlResponse interface that you have implemented. Here is an example: @DgsTypeResolver(name = \"PresignedUrlResponse\") public String resolvePresignedUrlResponse(PresignedUrlResponse response) { if (response instanceof UploadScriptResponse) { return \"UploadScriptResponse\"; } else { throw new RuntimeException(\"Invalid type: \" + response.getClass().getName() + \" found in PresignedUrlResponseResolver\"); } }","title":"Set up a resolver"},{"location":"advanced/file-uploads/#dgs-data-fetcher","text":"Your data fetcher in your DGS handles the above Mutation query by generating or requesting a pre-signed URL from Baggins, as shown here: @DgsData(parentType = DgsConstants.MUTATION.TYPE_NAME, field = \"uploadScriptWithPresignedUrl\") public UploadScriptResponse uploadScript(@InputArgument(\"input\") UploadScriptRequestInput uploadRequest, DataFetchingEnvironment dataFetchingEnvironment) throws IOException { BagginsService.GetTokenResponse response = bagginsService.getSignedUploadUrl(uploadRequest.getFilePath()); CompletableFuture<Void> future = CompletableFuture.runAsync(() -> { long timeout = 600000; long startTime = System.currentTimeMillis(); long elapsed = startTime; boolean status = false; while (!status && elapsed - startTime < timeout) { try { TimeUnit.SECONDS.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } status = bagginsService.checkFileUploadStatus(response.keyname); elapsed = System.currentTimeMillis() - startTime; } if (elapsed - startTime >= timeout) { logger.error(\"Encountered a timeout waiting for file %s to upload\", response.keyname); } // You can perform your post upload actions at this point - e.g., download the file for post processing // or process meta data try { bagginsService.downloadFile(response.keyname); } catch (IOException e) { e.printStackTrace(); } }); Integer fileSize = uploadRequest.getFileSize(); PresignedUrlResponse.Header header1 = new PresignedUrlResponse.Header(); header1.setName(\"Content-Type\"); header1.setValue(\"application/octet-stream\"); PresignedUrlResponse.Header header2 = new PresignedUrlResponse.Header(); header2.setName(\"x-baggins-upload-file-length\"); header2.setValue(fileSize.toString()); List<PresignedUrlResponse.Header> headers = new ArrayList<>(Arrays.asList(header1, header2)); return new UploadScriptResponse( response.token, headers, \"POST\", response.keyname); } To ensure consistency from a client perspective for file uploads and downloads, we recommend the following: Set up a CompletableFuture to poll on the status of the file upload by the client. Upon completion, carry out any post-processing, such as updating metadata or downloading the file for validation, etc. The client can check the status of the mutation operation (including metadata updates, etc.) via a separate status query. The input arguments can contain the header-specific information, such as the file size in case of using Baggins. The client can set these headers without having to know about the details of the file storage service. From the client perspective it is a URL to upload the file to, and any additional information (such as the headers and HTTP method to use) are opaque to the client.","title":"DGS Data Fetcher"},{"location":"advanced/file-uploads/#client-example","text":"In your client application, set up your Mutation query to request a pre-signed URL from the DGS. The client can use this URL to upload files directly to Baggins. After file uploads complete, the client can issue a subsequent query to the DGS if the client needs to check on the status of the mutation operation. For simple use cases, the client could just fire and forget. Such a second status query can be useful to the client in scenarios involving post-processing of the file by the DGS, such as validation of the file, or updating metadata in another service, e.g. Amsterdam . Here is an example client implementation showing the set up of queries in the UI application: // query for file uploads using pre-signed urls const PresignedUrlDetails_gql = gql` fragment PresignedUrlDetails on PresignedUrlResponse { url headers { name value } method } `; const UploadScriptRequestMutation_gql = gql` mutation uploadScriptWithPresignedUrl( $input: UploadScriptRequestInput ) { uploadScriptWithPresignedUrl(input: $input) { ...PresignedUrlDetails } } ${PresignedUrlDetails_gql} `; function ScriptUploadWithPresignedURL() { const [ scriptUpload, { loading: mutationLoading, error: mutationError, data: mutationData, }, ] = useMutation(UploadScriptRequestMutation_gql); const [scriptInput, setScriptInput] = useState<HTMLInputElement>(); type Header = { name: string; value: string; }; const uploadToUrl = ( url: string, headers: [Header], method: string, id: string, fileElement: HTMLInputElement ) => { if ( fileElement.type !== 'file' && fileElement.files!.length <= 0 ) { console.log('Incorrect type of file input'); return false; } const requestHeaders: HeadersInit = new Headers(); headers.forEach(iter => { requestHeaders.set(iter.name, iter.value); }); fetch(url, { method: method, headers: requestHeaders, body: fileElement.files![0], }) .then(result => { console.log(`Response from baggins ${result}`); }) .catch(error => { alert(`ERROR ${JSON.stringify(error)}`); console.log(`Received error ${JSON.stringify(error)}`); }); }; useEffect(() => { if (mutationData) { uploadToUrl( mutationData.uploadScriptWithPresignedUrl.url, mutationData.uploadScriptWithPresignedUrl.headers, mutationData.uploadScriptWithPresignedUrl.method, mutationData.uploadScriptWithPresignedUrl.uploadId, scriptInput! ); } }, [mutationData, scriptInput]); const onSubmitScriptPresigned = () => { const path = `${scriptInput!.files![0].name}`; const size = scriptInput!.files![0].size; scriptUpload({ variables: { input: { name: 'kavitha', description: 'script upload', filePath: path, fileSize: size, }, }, }); }; } The complete example is available in ScriptUploadWithPresignedURL.tsx from the file-uploads-example-ui project. --8<-- \"docs/reference_links\"","title":"Client Example"},{"location":"advanced/mocking-datafetchers/","text":"In the getting started tutorial you\u2019ll learn how to write tests for a data fetcher. This is useful for the developer of the data fetcher. However, what should you do if a UI developer wants to test against your schema? How do they test against a stable set of data? This guide is about how to provide mock data for data fetchers. There are two primary reasons to do so: Provide example data that UI teams can use while the data fetcher is under development. This is useful during schema design. Provide stable test data for UI teams to write their tests against. An argument can be made that this type of mock data should live in the UI code. It\u2019s for their tests after all. However, by pulling it into the [DGS], the owners of the data can provide test data that can be used by many teams. The two approaches are also not mutually exclusive. [GraphQL] Mocking The library in the [DGS] framework supports: returning static data from mocks returning generated data for simple types (like String fields) remotely enabling mocks for a [Meechum] user by using [Simone] The library is modular, so you can use it for a variety of workflows and use cases. The mocking framework is already part of the DGS framework. All you need to provide is one or more MockProvider implementations. MockProvider is an interface with a Map<String, Object> provide() method. Each key in the Map is a field in the [GraphQL] schema, which can be several levels deep. The value in the Map is whatever mock data you want to return for this key. Example Create a MockProvider that provides mock data for the hello field you created in the getting started tutorial : @Component public class HelloMockProvider implements MockProvider { @NotNull @Override public Map<String, Object> provide() { Map<String, Object> mock = new HashMap<>(); mock.put(\"hello\", \"Mocked hello response\"); return mock; } } If you run the application again and test the hello query, you will see that it now returns the mock data. This is useful while a data fetcher isn\u2019t implemented yet, but the true power comes from enabling the mocks remotely. [Simone] Integration Simone is a distributed testing tool that testers use in the streaming world for end-to-end testing and device certification. A micro-service integrates with the Simone client, and a tester can activate test behavior by creating a Variant by using either the UI or an API. By creating Variant s, a tester can trigger special behavior in a service in order to validate very specific scenarios. The [DGS] framework integrates with Simone to enable a MockProvider when a Variant is active for a specific user email address. This way, a tester or UI developer can enable mock behavior for a specific account, while the mock data itself is owned by the DGS owner. Change the HelloMockProvider implementation to use the Simone integration as follows: @Component public class HelloMockProvider implements MockProvider { @Autowired SimoneSsoMockProvider simoneSsoMockProvider; public Map<String, Object> provide() { return simoneSsoMockProvider.provide((identity, variant) -> { HashMap<String, Object> mocks = Maps.newHashMap(); mocks.put(\"hello\", \"Hello from Simone, \" + identity); return mocks; }, SsoCaller::getName); } } SimoneSsoMockProvider is a thin wrapper for Simone. It picks up the SsoCaller that\u2019s available in [Spring Boot], and checks Simone to see if a variant is active for the caller\u2019s email address. The code in the lambda passed to the provide method only executes if a variant for the user is available. Before you create such a variant and test it out, you must perform some configuration for the Simone [gRPC] client. grpc: client: okja: OkjaService: channel: target: eureka:///okjagrpctest usePlaintext: false negotiationType: TLS sslContextFactory: metatron targetApplication: okja interceptor: retry: default: maxRetries: 3 statuses: UNAVAILABLE dgs: mocking: simone: enabled: true With the code and configuration in place, the next query you send to hello should give the same result as it previously did. Simone will only become active after you create a variant. The easiest way to create a variant is to use the Simone UI: go/simone . Click Variants \u2192 Create Variant and select the TEST environment. A form appears that lets you create a variant. Select com.netflix.graphql.mocking as the Template . Eviction Count indicates the number of times Simone will apply this variant before it expires. For now, set it to 10, so that you can run a few tests. The Correlation Id is an ID that you can pick, which you can later use to search Simone\u2019s variant tracing. The Trigger type is com.netflix.simone.trigger.esnExactMatch . An ESN is something from the streaming world; the DGS framework uses the user\u2019s email address instead. Set the value of ESN to your [Meechum] email address. After you create the variant by clicking Create , the next query you send to hello should generate a response from the Simone mock. The com.netflix.graphql.mocking template allows for an arbitrary object with key/values to be passed as argument data when creating the variant. The variant arguments are available in code, and this allows for some dynamic behavior in mocks. The following is an example that uses Jackson to parse the argument data: @Component public class HelloMockProvider implements MockProvider { @Autowired Optional<SimoneSsoMockProvider> simoneSsoMockProvider; public Map<String, Object> provide() { return simoneSsoMockProvider.map(ssoMockProvider -> ssoMockProvider.provide((identity, variant) -> { HashMap<String, Object> mocks = Maps.newHashMap(); ObjectMapper mapper = new ObjectMapper(); try { Map<String, String> args = mapper.readValue(variant.getArgumentData(), new TypeReference<Map<String, String>>() { }); mocks.put(\"hello\", \"Hello from Simone, \" + identity + \". MyArg: \" + args.get(\"myarg\")); System.out.println(args); } catch (IOException e) { e.printStackTrace(); } return mocks; }, SsoCaller::getName)).orElse(Collections.emptyMap()); } } To learn more about Simone, and options to use its APIs to create Variants, refer to the Simone Documentation . Mock Return Types In the previous examples you have implemented mocks with static data, where the provide() method returns a static set of key/values. In addition to hardcoded values, the mock framework also supports returning a data fetcher (which gives access to the DataFetchingEnvironment ) and partly-generated data. The following example mocks the hello field, but doesn\u2019t provide a value. The mock framework will generate data based on the object type of the field defined in the schema. This is also an effective way to mock arrays of data. When the schema defines a field as an array, the mock framework generates an array of variable size. public Map<String, Object> provide() { Map<String, Object> mocks = new HashMap<>(); mocks.put(\"hello\", null); return mocks; } You can also provide a data fetcher as the value of the mock. This gives access to the DataFetchingEnvironment , so that you can, for example, use arguments in the generated mock data: public Map<String, Object> provide() { Map<String, Object> mocks = new HashMap<>(); DataFetcher datafetcher = (dataFetchingEnvironment) -> \"Hello from mock, \" + dataFetchingEnvironment.getArgument(\"name\"); mocks.put(\"hello\", datafetcher); return mocks; } --8<-- \"docs/reference_links\"","title":"Mocking datafetchers"},{"location":"advanced/mocking-datafetchers/#graphql-mocking","text":"The library in the [DGS] framework supports: returning static data from mocks returning generated data for simple types (like String fields) remotely enabling mocks for a [Meechum] user by using [Simone] The library is modular, so you can use it for a variety of workflows and use cases. The mocking framework is already part of the DGS framework. All you need to provide is one or more MockProvider implementations. MockProvider is an interface with a Map<String, Object> provide() method. Each key in the Map is a field in the [GraphQL] schema, which can be several levels deep. The value in the Map is whatever mock data you want to return for this key.","title":"[GraphQL] Mocking"},{"location":"advanced/mocking-datafetchers/#example","text":"Create a MockProvider that provides mock data for the hello field you created in the getting started tutorial : @Component public class HelloMockProvider implements MockProvider { @NotNull @Override public Map<String, Object> provide() { Map<String, Object> mock = new HashMap<>(); mock.put(\"hello\", \"Mocked hello response\"); return mock; } } If you run the application again and test the hello query, you will see that it now returns the mock data. This is useful while a data fetcher isn\u2019t implemented yet, but the true power comes from enabling the mocks remotely.","title":"Example"},{"location":"advanced/mocking-datafetchers/#simone-integration","text":"Simone is a distributed testing tool that testers use in the streaming world for end-to-end testing and device certification. A micro-service integrates with the Simone client, and a tester can activate test behavior by creating a Variant by using either the UI or an API. By creating Variant s, a tester can trigger special behavior in a service in order to validate very specific scenarios. The [DGS] framework integrates with Simone to enable a MockProvider when a Variant is active for a specific user email address. This way, a tester or UI developer can enable mock behavior for a specific account, while the mock data itself is owned by the DGS owner. Change the HelloMockProvider implementation to use the Simone integration as follows: @Component public class HelloMockProvider implements MockProvider { @Autowired SimoneSsoMockProvider simoneSsoMockProvider; public Map<String, Object> provide() { return simoneSsoMockProvider.provide((identity, variant) -> { HashMap<String, Object> mocks = Maps.newHashMap(); mocks.put(\"hello\", \"Hello from Simone, \" + identity); return mocks; }, SsoCaller::getName); } } SimoneSsoMockProvider is a thin wrapper for Simone. It picks up the SsoCaller that\u2019s available in [Spring Boot], and checks Simone to see if a variant is active for the caller\u2019s email address. The code in the lambda passed to the provide method only executes if a variant for the user is available. Before you create such a variant and test it out, you must perform some configuration for the Simone [gRPC] client. grpc: client: okja: OkjaService: channel: target: eureka:///okjagrpctest usePlaintext: false negotiationType: TLS sslContextFactory: metatron targetApplication: okja interceptor: retry: default: maxRetries: 3 statuses: UNAVAILABLE dgs: mocking: simone: enabled: true With the code and configuration in place, the next query you send to hello should give the same result as it previously did. Simone will only become active after you create a variant. The easiest way to create a variant is to use the Simone UI: go/simone . Click Variants \u2192 Create Variant and select the TEST environment. A form appears that lets you create a variant. Select com.netflix.graphql.mocking as the Template . Eviction Count indicates the number of times Simone will apply this variant before it expires. For now, set it to 10, so that you can run a few tests. The Correlation Id is an ID that you can pick, which you can later use to search Simone\u2019s variant tracing. The Trigger type is com.netflix.simone.trigger.esnExactMatch . An ESN is something from the streaming world; the DGS framework uses the user\u2019s email address instead. Set the value of ESN to your [Meechum] email address. After you create the variant by clicking Create , the next query you send to hello should generate a response from the Simone mock. The com.netflix.graphql.mocking template allows for an arbitrary object with key/values to be passed as argument data when creating the variant. The variant arguments are available in code, and this allows for some dynamic behavior in mocks. The following is an example that uses Jackson to parse the argument data: @Component public class HelloMockProvider implements MockProvider { @Autowired Optional<SimoneSsoMockProvider> simoneSsoMockProvider; public Map<String, Object> provide() { return simoneSsoMockProvider.map(ssoMockProvider -> ssoMockProvider.provide((identity, variant) -> { HashMap<String, Object> mocks = Maps.newHashMap(); ObjectMapper mapper = new ObjectMapper(); try { Map<String, String> args = mapper.readValue(variant.getArgumentData(), new TypeReference<Map<String, String>>() { }); mocks.put(\"hello\", \"Hello from Simone, \" + identity + \". MyArg: \" + args.get(\"myarg\")); System.out.println(args); } catch (IOException e) { e.printStackTrace(); } return mocks; }, SsoCaller::getName)).orElse(Collections.emptyMap()); } } To learn more about Simone, and options to use its APIs to create Variants, refer to the Simone Documentation .","title":"[Simone] Integration"},{"location":"advanced/mocking-datafetchers/#mock-return-types","text":"In the previous examples you have implemented mocks with static data, where the provide() method returns a static set of key/values. In addition to hardcoded values, the mock framework also supports returning a data fetcher (which gives access to the DataFetchingEnvironment ) and partly-generated data. The following example mocks the hello field, but doesn\u2019t provide a value. The mock framework will generate data based on the object type of the field defined in the schema. This is also an effective way to mock arrays of data. When the schema defines a field as an array, the mock framework generates an array of variable size. public Map<String, Object> provide() { Map<String, Object> mocks = new HashMap<>(); mocks.put(\"hello\", null); return mocks; } You can also provide a data fetcher as the value of the mock. This gives access to the DataFetchingEnvironment , so that you can, for example, use arguments in the generated mock data: public Map<String, Object> provide() { Map<String, Object> mocks = new HashMap<>(); DataFetcher datafetcher = (dataFetchingEnvironment) -> \"Hello from mock, \" + dataFetchingEnvironment.getArgument(\"name\"); mocks.put(\"hello\", datafetcher); return mocks; } --8<-- \"docs/reference_links\"","title":"Mock Return Types"},{"location":"advanced/react-app-development/","text":"[Spring Boot] at Netflix supports serving static assets with [Meechum] integration out-of-the-box. The [DGS] framework leverages this mechanism and so you can use it to host web apps, if you need to do this in addition to implementing a [GraphQL] API. The following sections describe how to set up a create-react-app for test, production, and local development. Test and Production The DGS, by default, will serve any static assets in resources/public . If your app and the DGS reside in the same repository, one approach is for you to integrate the steps for building your app and copying them to resources/public within your Rocket CI script: cd ./react-app-dir && newt exec yarn run build && cd .. cp -r ./react-app-dir/build/ src/main/resources/public After you deploy the DGS you can access your deployed app on port 8443. Note that this does not require the app to set up the authorization header to access the /graphql endpoint, since Spring Boot apps are already integrated with Meechum. !!!note If you choose to use a different backend for serving static assets, you will need to set up Meechum authentication in your React app. Local Development If you want to run your react app locally by executing yarn start or npm start , and you want to avoid building and copying the app to /resources/public , you will need to explictly set up Meechum authentication. On the DGS, you will need a CORS filter that allows requests from a host that is different from your DGS: @Configuration public class LocalHostSecurityConfig { private static final Logger LOGGER = LoggerFactory.getLogger(LocalHostSecurityConfig.class); @Bean public FilterRegistrationBean corsFilter() { LOGGER.info(\"Setting up custom CORS\"); UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); CorsConfiguration config = new CorsConfiguration(); config.addAllowedOrigin(\"http://localhost:3000\"); // yarn start config.addAllowedOrigin(\"https://localhost:8444\"); // meego config.addAllowedHeader(\"Accept\"); config.addAllowedHeader(\"Content-Type\"); config.addAllowedHeader(\"X-Requested-With\"); config.addAllowedHeader(\"Authorization\"); config.addAllowedMethod(\"GET\"); config.addAllowedMethod(\"POST\"); config.addAllowedMethod(\"OPTIONS\"); source.registerCorsConfiguration(\"/**\", config); FilterRegistrationBean bean = new FilterRegistrationBean(new CorsFilter(source)); return bean; } } On the client side, you will need to set up the following: Add meechum-user-lib-js as a dependency by setting up your .npmrc or .yarnc with registry = https://repo.test.netflix.net/artifactory/api/npm/npm-netflix . Use the following snippet to set up Meechum authentication in your app: await MeechumUser.initialize( '/meechum', 900, () => console.warn('Warning: Meechum session timed out'), () => console.error(`Error: Refreshing meechum token failed`)); const token = Meechum.getUserToken(); You can now use the token and can set that in your authorization header for the request to /graphql endpoint. Run meego . This will proxy your request to your local node server. For example: meego -listenPort 8444 -proxyTarget http://localhost:3000 -clientId yourEdwardPolicyClientId -clientSecret yourEdwardPolicyClientSecret You can now start your react app by executing yarn start or npm start as usual. Your app can make requests to the /graphql endpoint on the DGS. This allows the developer to update the UI without having to restart the DGS each time, in cases where the DGS also serves static assets. --8<-- \"docs/reference_links\"","title":"React app development"},{"location":"advanced/react-app-development/#test-and-production","text":"The DGS, by default, will serve any static assets in resources/public . If your app and the DGS reside in the same repository, one approach is for you to integrate the steps for building your app and copying them to resources/public within your Rocket CI script: cd ./react-app-dir && newt exec yarn run build && cd .. cp -r ./react-app-dir/build/ src/main/resources/public After you deploy the DGS you can access your deployed app on port 8443. Note that this does not require the app to set up the authorization header to access the /graphql endpoint, since Spring Boot apps are already integrated with Meechum. !!!note If you choose to use a different backend for serving static assets, you will need to set up Meechum authentication in your React app.","title":"Test and Production"},{"location":"advanced/react-app-development/#local-development","text":"If you want to run your react app locally by executing yarn start or npm start , and you want to avoid building and copying the app to /resources/public , you will need to explictly set up Meechum authentication. On the DGS, you will need a CORS filter that allows requests from a host that is different from your DGS: @Configuration public class LocalHostSecurityConfig { private static final Logger LOGGER = LoggerFactory.getLogger(LocalHostSecurityConfig.class); @Bean public FilterRegistrationBean corsFilter() { LOGGER.info(\"Setting up custom CORS\"); UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); CorsConfiguration config = new CorsConfiguration(); config.addAllowedOrigin(\"http://localhost:3000\"); // yarn start config.addAllowedOrigin(\"https://localhost:8444\"); // meego config.addAllowedHeader(\"Accept\"); config.addAllowedHeader(\"Content-Type\"); config.addAllowedHeader(\"X-Requested-With\"); config.addAllowedHeader(\"Authorization\"); config.addAllowedMethod(\"GET\"); config.addAllowedMethod(\"POST\"); config.addAllowedMethod(\"OPTIONS\"); source.registerCorsConfiguration(\"/**\", config); FilterRegistrationBean bean = new FilterRegistrationBean(new CorsFilter(source)); return bean; } } On the client side, you will need to set up the following: Add meechum-user-lib-js as a dependency by setting up your .npmrc or .yarnc with registry = https://repo.test.netflix.net/artifactory/api/npm/npm-netflix . Use the following snippet to set up Meechum authentication in your app: await MeechumUser.initialize( '/meechum', 900, () => console.warn('Warning: Meechum session timed out'), () => console.error(`Error: Refreshing meechum token failed`)); const token = Meechum.getUserToken(); You can now use the token and can set that in your authorization header for the request to /graphql endpoint. Run meego . This will proxy your request to your local node server. For example: meego -listenPort 8444 -proxyTarget http://localhost:3000 -clientId yourEdwardPolicyClientId -clientSecret yourEdwardPolicyClientSecret You can now start your react app by executing yarn start or npm start as usual. Your app can make requests to the /graphql endpoint on the DGS. This allows the developer to update the UI without having to restart the DGS each time, in cases where the DGS also serves static assets. --8<-- \"docs/reference_links\"","title":"Local Development"},{"location":"advanced/schema-from-code/","text":"Use a schema-first approach for [GraphQL] in most cases. Most [DGS]s have a schema file and use the declarative, annotation based, programming model to create data fetchers and such. Creating a Schema from Code during Startup There are scenarios however in which it is better to create a schema from code, during startup of the application. An example is a GraphQL schema which is derived from another schema, or in code generation scenarios. The DGS framework supports this by letting users register a TypeDefinitionRegistry and GraphQLCodeRegistry , both concepts from the graphql-java library. The TypeDefinitionRegistry represents a schema and GraphQLCodeRegistry the mapping from fields to data fetchers. Both are concepts from the graphql-java core library. The types defined in a provided TypeDefinitionRegistry are merged with any schema files when available . A combination of data fetchers registered using annotations and a GraphQLCodeRegistry works as well. The following is an example of creating a TypeDefinitionRegistry . @Configuration public class ExtraTypeDefinitionRegistry { @Bean public TypeDefinitionRegistry registry() { ObjectTypeExtensionDefinition objectTypeExtensionDefinition = ObjectTypeExtensionDefinition.newObjectTypeExtensionDefinition().name(\"Query\").fieldDefinition(FieldDefinition.newFieldDefinition().name(\"myField\").type(new TypeName(\"String\")).build()) .build(); TypeDefinitionRegistry typeDefinitionRegistry = new TypeDefinitionRegistry(); typeDefinitionRegistry.add(objectTypeExtensionDefinition); return typeDefinitionRegistry; } } This TypeDefinitionRegistry creates a field myField on the Query object type. This means there also needs to be a datafetcher for this field, which is created using the @DgsCodeRegistry annotation. @DgsComponent public class ExtraCodeRegistry { @Autowired DgsDataFetcherFactory dgsDataFetcherFactory; @DgsCodeRegistry public GraphQLCodeRegistry.Builder registry(GraphQLCodeRegistry.Builder codeRegistryBuilder, TypeDefinitionRegistry registry) { DataFetcher<String> df = (dfe) -> \"yes, my extra field!\"; FieldCoordinates coordinates = FieldCoordinates.coordinates(\"Query\", \"myField\"); DataFetcher<Object> dgsDataFetcher = dgsDataFetcherFactory.createDataFetcher(coordinates, df); return codeRegistryBuilder.dataFetcher(coordinates, dgsDataFetcher); } } Note how this example calls a method of the DgsDataFetcherFactory to create the data fetcher. By using this factory, the data fetcher gets all the extra features that DGS data fetchers have, such as tracing. Generating a Schema from Java or Kotlin Classes An alternative way to start from code is to convert the classes from your existing Java or Kotlin code into GraphQL schemas with the GraphQL Schema Generator . This tool is good enough to generate an initial schema that conforms to your classes, but you will then need to manually go through the generated schema and improve or correct it. REST or [gRPC] entities do not always map well to GraphQL types: graphql-code-to-schema Installation and Use Instructions --8<-- \"docs/reference_links\"","title":"Schema from code"},{"location":"advanced/schema-from-code/#creating-a-schema-from-code-during-startup","text":"There are scenarios however in which it is better to create a schema from code, during startup of the application. An example is a GraphQL schema which is derived from another schema, or in code generation scenarios. The DGS framework supports this by letting users register a TypeDefinitionRegistry and GraphQLCodeRegistry , both concepts from the graphql-java library. The TypeDefinitionRegistry represents a schema and GraphQLCodeRegistry the mapping from fields to data fetchers. Both are concepts from the graphql-java core library. The types defined in a provided TypeDefinitionRegistry are merged with any schema files when available . A combination of data fetchers registered using annotations and a GraphQLCodeRegistry works as well. The following is an example of creating a TypeDefinitionRegistry . @Configuration public class ExtraTypeDefinitionRegistry { @Bean public TypeDefinitionRegistry registry() { ObjectTypeExtensionDefinition objectTypeExtensionDefinition = ObjectTypeExtensionDefinition.newObjectTypeExtensionDefinition().name(\"Query\").fieldDefinition(FieldDefinition.newFieldDefinition().name(\"myField\").type(new TypeName(\"String\")).build()) .build(); TypeDefinitionRegistry typeDefinitionRegistry = new TypeDefinitionRegistry(); typeDefinitionRegistry.add(objectTypeExtensionDefinition); return typeDefinitionRegistry; } } This TypeDefinitionRegistry creates a field myField on the Query object type. This means there also needs to be a datafetcher for this field, which is created using the @DgsCodeRegistry annotation. @DgsComponent public class ExtraCodeRegistry { @Autowired DgsDataFetcherFactory dgsDataFetcherFactory; @DgsCodeRegistry public GraphQLCodeRegistry.Builder registry(GraphQLCodeRegistry.Builder codeRegistryBuilder, TypeDefinitionRegistry registry) { DataFetcher<String> df = (dfe) -> \"yes, my extra field!\"; FieldCoordinates coordinates = FieldCoordinates.coordinates(\"Query\", \"myField\"); DataFetcher<Object> dgsDataFetcher = dgsDataFetcherFactory.createDataFetcher(coordinates, df); return codeRegistryBuilder.dataFetcher(coordinates, dgsDataFetcher); } } Note how this example calls a method of the DgsDataFetcherFactory to create the data fetcher. By using this factory, the data fetcher gets all the extra features that DGS data fetchers have, such as tracing.","title":"Creating a Schema from Code during Startup"},{"location":"advanced/schema-from-code/#generating-a-schema-from-java-or-kotlin-classes","text":"An alternative way to start from code is to convert the classes from your existing Java or Kotlin code into GraphQL schemas with the GraphQL Schema Generator . This tool is good enough to generate an initial schema that conforms to your classes, but you will then need to manually go through the generated schema and improve or correct it. REST or [gRPC] entities do not always map well to GraphQL types: graphql-code-to-schema Installation and Use Instructions --8<-- \"docs/reference_links\"","title":"Generating a Schema from Java or Kotlin Classes"},{"location":"advanced/security/","text":"By default a [Gandalf] policy protects every [DGS]. This policy only allows access via [GraphQL] for employees. This prevents accidental exposure of data to the outside world via the [Studio Edge Gateway]. The Gandalf policy is checked for each incoming request. If the policy check fails a 403 is returned . The default Gandalf policy is studio-edge-dgs-default . You can see the details of the policy here . It includes the following users: all-netflix-employees This is a safe default, but for many Studio use cases it is not sufficient. For example, contractors may need access to the data, or you require finer control for specific fields in the graph. The framework makes it easy to specify your own policies both on a coarse- and a fine-grained level. Creating Your Own Policy To change the coarse-grained check, create your own [Gandalf] policy. Once you create the policy, hook it up to the [DGS] framework by adding the following configuration in application.yml : dgs: security: policyname: your-gandalf-policy Fine-grained Access Control with @Secured To establish fine-grained access control on data fetchers, apply the standard Spring @Secured annotation. This allows stricter checks for specific fields in the graph. @DgsComponent public class SecurityExampleFetchers { @DgsData(parentType = \"Query\", field = \"hello\") public String hello() { return \"Hello to everyone passing the coarse grained policy\"; } @Secured(\"studio-edge-dgs-examples-deny\") @DgsData(parentType = \"Query\", field = \"deny\") public String deny() { return \"this shouldn't show\"; } @Secured(\"studio-edge-dgs-users\") @DgsData(parentType = \"Query\", field = \"secureGroup\") public String secureGroup() { return \"Only for users in studio-edge-dgs-users\"; } } This works the same as for [gRPC] and REST in Spring, including role mappings. See Spring Reference: Security and Secrets: @Secured Access Control . Note that a policy in @Secured can only enforce stricter control; it cannot loosen such control. A request first goes through the coarse-grained checks before it even checks the data fetchers. !!!caution The @Secured annotation will not protect a function that is called by another unsecured function within the same class. In other words, if foo() calls @Secured bar() but foo() is not marked @Secured , the security will not be applied to bar() during that call either. `@Secured` also fails to work for data loaders. As a workaround, you can use<!-- http://go/use --> `SsoCaller` in data loaders. Programmatic Access Control A [DGS] is just a [Spring Boot] app, so you can still use the security integration Netflix has for any Spring Boot app. An example is injecting the SsoCaller to get information about the calling user/app: @DgsComponent public class HelloDataFetcher { @Autowired SsoCallerResolver ssoCallerResolver; @DgsData(parentType = \"Query\", field = \"hello\") public String hello(DataFetchingEnvironment dfe) { //Note that these values are all optionals, e.g. different fields are there for Metatron calls! String fullname = ssoCallerResolver.get().getUser().get().getFullName().get(); return \"Hello, \" + fullname; } } More documentation can be found in Spring Reference: Security and Secrets: SsoCaller Usage . Testing with Security Query testing with @Secured is easy too. By default, tests ignore the @Secured annotation while unit testing, so no additional setup is required. If you want to test SSO features specifically (for example if your data fetcher relies on SSO features), [Spring Boot] has excellent support for doing so. The following example [DGS] test tests if an @Secured data fetcher only allows requests from users in the correct group. @SpringBootTest(classes = {SecurityExampleFetchers.class, DgsAutoConfiguration.class}) @EnableSsoTest public class SecureDataFetcherTest { @Autowired DgsQueryExecutor queryExecutor; @Test @WithSsoUser(name = \"validuser\", gandalfPolicies = \"studio-edge-dgs-users\") public void testSecureWithValidUser() { ExecutionResult executionResult = queryExecutor.execute(\"{secureGroup}\"); assertNotNull(executionResult); assertTrue(executionResult.isDataPresent()); assertEquals(0, executionResult.getErrors().size()); } @Test @WithSsoUser(name = \"invaliduser\") public void testSecured() { ExecutionResult executionResult = queryExecutor.execute(\"{secureGroup}\"); assertNotNull(executionResult); assertEquals(1, executionResult.getErrors().size()); assertEquals(\"org.springframework.security.access.AccessDeniedException: Access is denied\", executionResult.getErrors().get(0).getMessage()); } } Switching Off Gandalf To completely switch off [AuthZ], you can use the following configuration: dgs: security: disabled: true --8<-- \"docs/reference_links\"","title":"Security"},{"location":"advanced/security/#creating-your-own-policy","text":"To change the coarse-grained check, create your own [Gandalf] policy. Once you create the policy, hook it up to the [DGS] framework by adding the following configuration in application.yml : dgs: security: policyname: your-gandalf-policy","title":"Creating Your Own Policy"},{"location":"advanced/security/#fine-grained-access-control-with-secured","text":"To establish fine-grained access control on data fetchers, apply the standard Spring @Secured annotation. This allows stricter checks for specific fields in the graph. @DgsComponent public class SecurityExampleFetchers { @DgsData(parentType = \"Query\", field = \"hello\") public String hello() { return \"Hello to everyone passing the coarse grained policy\"; } @Secured(\"studio-edge-dgs-examples-deny\") @DgsData(parentType = \"Query\", field = \"deny\") public String deny() { return \"this shouldn't show\"; } @Secured(\"studio-edge-dgs-users\") @DgsData(parentType = \"Query\", field = \"secureGroup\") public String secureGroup() { return \"Only for users in studio-edge-dgs-users\"; } } This works the same as for [gRPC] and REST in Spring, including role mappings. See Spring Reference: Security and Secrets: @Secured Access Control . Note that a policy in @Secured can only enforce stricter control; it cannot loosen such control. A request first goes through the coarse-grained checks before it even checks the data fetchers. !!!caution The @Secured annotation will not protect a function that is called by another unsecured function within the same class. In other words, if foo() calls @Secured bar() but foo() is not marked @Secured , the security will not be applied to bar() during that call either. `@Secured` also fails to work for data loaders. As a workaround, you can use<!-- http://go/use --> `SsoCaller` in data loaders.","title":"Fine-grained Access Control with @Secured"},{"location":"advanced/security/#programmatic-access-control","text":"A [DGS] is just a [Spring Boot] app, so you can still use the security integration Netflix has for any Spring Boot app. An example is injecting the SsoCaller to get information about the calling user/app: @DgsComponent public class HelloDataFetcher { @Autowired SsoCallerResolver ssoCallerResolver; @DgsData(parentType = \"Query\", field = \"hello\") public String hello(DataFetchingEnvironment dfe) { //Note that these values are all optionals, e.g. different fields are there for Metatron calls! String fullname = ssoCallerResolver.get().getUser().get().getFullName().get(); return \"Hello, \" + fullname; } } More documentation can be found in Spring Reference: Security and Secrets: SsoCaller Usage .","title":"Programmatic Access Control"},{"location":"advanced/security/#testing-with-security","text":"Query testing with @Secured is easy too. By default, tests ignore the @Secured annotation while unit testing, so no additional setup is required. If you want to test SSO features specifically (for example if your data fetcher relies on SSO features), [Spring Boot] has excellent support for doing so. The following example [DGS] test tests if an @Secured data fetcher only allows requests from users in the correct group. @SpringBootTest(classes = {SecurityExampleFetchers.class, DgsAutoConfiguration.class}) @EnableSsoTest public class SecureDataFetcherTest { @Autowired DgsQueryExecutor queryExecutor; @Test @WithSsoUser(name = \"validuser\", gandalfPolicies = \"studio-edge-dgs-users\") public void testSecureWithValidUser() { ExecutionResult executionResult = queryExecutor.execute(\"{secureGroup}\"); assertNotNull(executionResult); assertTrue(executionResult.isDataPresent()); assertEquals(0, executionResult.getErrors().size()); } @Test @WithSsoUser(name = \"invaliduser\") public void testSecured() { ExecutionResult executionResult = queryExecutor.execute(\"{secureGroup}\"); assertNotNull(executionResult); assertEquals(1, executionResult.getErrors().size()); assertEquals(\"org.springframework.security.access.AccessDeniedException: Access is denied\", executionResult.getErrors().get(0).getMessage()); } }","title":"Testing with Security"},{"location":"advanced/security/#switching-off-gandalf","text":"To completely switch off [AuthZ], you can use the following configuration: dgs: security: disabled: true --8<-- \"docs/reference_links\"","title":"Switching Off Gandalf"},{"location":"advanced/subscriptions/","text":"[GraphQL] Subscriptions are used to receive updates for a query from the server over time. A common example is sending update notifications from the server. Regular GraphQL queries use a simple (HTTP) request/response to execute a query. For subscriptions a connection is kept open. Depending on how a DGS is deployed , there are different options to set this up. The programming model for a DGS developer is the same regardless of the transport protocol, but the correct transport has to be selected to successfully run within Netflix infrastructure. The following choices are available: WebSockets (no [Wall-E] support, internal tooling only) SSE (Wall-E supported, apps outside VPN supported) [Federated] Gateway (for DGSs behind the federated gateway) The Server Side Programming Model In the DGS framework a Subscription is implemented as a data fetcher with the @DgsData annotation. The difference with a normal data fetcher is that a subscription must return a org.reactivestreams.Publisher . import reactor.core.publisher.Flux; import org.reactivestreams.Publisher; \u22ee @DgsData(parentType = \"Subscription\", field = \"stocks\") public Publisher<Stock> stocks() { return Flux.interval(Duration.ofSeconds(1)).map({ t -> Tick(t.toString()) }) } The Publisher interface is from Reactive Streams. Flux is the default implementation for Spring. A complete example can be found in SubscriptionDatafetcher.kt from dgs-demo-minimal . Next, a transport implementation must be chosen , which depends on how your app is deployed . WebSockets The most common transport protocol for Subscriptions in the GraphQL community is WebSockets. Apollo defines a sub-protocol , which is supported by client libraries and implemented by the DGS framework. However, WebSockets are problematic for proxies, including Wall-E. If your backend is behind Wall-E, WebSockets are not supported . WebSockets work great if your DGS is for an internal tool, where the UI (which can be hosted on Treasure ) connects to the backend directly using Eureka DNS. To enable WebSockets support, add the following module to your build.gradle : implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure:latest.release' Apollo client supports WebSockets through a link . Typically you want to configure Apollo Client with both an HTTP link and a WS link, and split between them based on the query type. SSE Server Send Events are the right choice if your backend is proxied by Wall-E. To enable SSE support, add the following module to your build.gradle : implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-sse-autoconfigure:latest.release' An internal TypeScript module, nflx-subscriptions-transport-sse , supports Apollo Link. This module reimplements an (unmaintained) open source library . The reimplementation was required to support Wall-E. From a client-side perspective the usage is the same as for the WebSocket implementation, just with a different module: import { SubscriptionClient, SSELink} from 'nflx-subscriptions-transport-sse' const sseClient = new SubscriptionClient(SUBSCRIPTION_ENDPOINT); const sseLink = new SSELink(sseClient); const authedClient = authLink && new ApolloClient({ link: authLink.concat(split((operation) => { return operation.operationName === \"TicksWatch\"}, sseLink, httpLink)), cache: new InMemoryCache() }) Studio Edge Gateway Studio Edge Gateway supports GraphQL subscriptions. A client sets up a WSS or SSE connection with the gateway, and the gateway uses a proprietary protocol based on callbacks with the DGS. Note that you don't deal with this protocol directly as a DGS developer, the framework takes care of this! To enable gateway subscriptions, add the following module to your build.gradle : implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-gateway-autoconfigure:latest.release' The programming model for the DGS developer is exactly the same as described above for SSE/WebSockets, using the Publisher interface with Flux. The gateway supports both SSE and WebSockets from clients using the client libraries discussed above. !!!info \"Note on gateway subscriptions\" There are currently a few caveats when using subscriptions through a gateway. 1) Don\u2019t deliver sensitive data through a subscription connection. If you need to deliver sensitive data, the subscription event can trigger the UI to make a followup query. 2) The subscription can\u2019t return a federated type. The same DGS must provide the entire event payload. 3) Expected volume should be <1000 concurrent connections --8<-- \"docs/reference_links\"","title":"Subscriptions"},{"location":"advanced/subscriptions/#the-server-side-programming-model","text":"In the DGS framework a Subscription is implemented as a data fetcher with the @DgsData annotation. The difference with a normal data fetcher is that a subscription must return a org.reactivestreams.Publisher . import reactor.core.publisher.Flux; import org.reactivestreams.Publisher; \u22ee @DgsData(parentType = \"Subscription\", field = \"stocks\") public Publisher<Stock> stocks() { return Flux.interval(Duration.ofSeconds(1)).map({ t -> Tick(t.toString()) }) } The Publisher interface is from Reactive Streams. Flux is the default implementation for Spring. A complete example can be found in SubscriptionDatafetcher.kt from dgs-demo-minimal . Next, a transport implementation must be chosen , which depends on how your app is deployed .","title":"The Server Side Programming Model"},{"location":"advanced/subscriptions/#websockets","text":"The most common transport protocol for Subscriptions in the GraphQL community is WebSockets. Apollo defines a sub-protocol , which is supported by client libraries and implemented by the DGS framework. However, WebSockets are problematic for proxies, including Wall-E. If your backend is behind Wall-E, WebSockets are not supported . WebSockets work great if your DGS is for an internal tool, where the UI (which can be hosted on Treasure ) connects to the backend directly using Eureka DNS. To enable WebSockets support, add the following module to your build.gradle : implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure:latest.release' Apollo client supports WebSockets through a link . Typically you want to configure Apollo Client with both an HTTP link and a WS link, and split between them based on the query type.","title":"WebSockets"},{"location":"advanced/subscriptions/#sse","text":"Server Send Events are the right choice if your backend is proxied by Wall-E. To enable SSE support, add the following module to your build.gradle : implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-sse-autoconfigure:latest.release' An internal TypeScript module, nflx-subscriptions-transport-sse , supports Apollo Link. This module reimplements an (unmaintained) open source library . The reimplementation was required to support Wall-E. From a client-side perspective the usage is the same as for the WebSocket implementation, just with a different module: import { SubscriptionClient, SSELink} from 'nflx-subscriptions-transport-sse' const sseClient = new SubscriptionClient(SUBSCRIPTION_ENDPOINT); const sseLink = new SSELink(sseClient); const authedClient = authLink && new ApolloClient({ link: authLink.concat(split((operation) => { return operation.operationName === \"TicksWatch\"}, sseLink, httpLink)), cache: new InMemoryCache() })","title":"SSE"},{"location":"advanced/subscriptions/#studio-edge-gateway","text":"Studio Edge Gateway supports GraphQL subscriptions. A client sets up a WSS or SSE connection with the gateway, and the gateway uses a proprietary protocol based on callbacks with the DGS. Note that you don't deal with this protocol directly as a DGS developer, the framework takes care of this! To enable gateway subscriptions, add the following module to your build.gradle : implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-gateway-autoconfigure:latest.release' The programming model for the DGS developer is exactly the same as described above for SSE/WebSockets, using the Publisher interface with Flux. The gateway supports both SSE and WebSockets from clients using the client libraries discussed above. !!!info \"Note on gateway subscriptions\" There are currently a few caveats when using subscriptions through a gateway. 1) Don\u2019t deliver sensitive data through a subscription connection. If you need to deliver sensitive data, the subscription event can trigger the UI to make a followup query. 2) The subscription can\u2019t return a federated type. The same DGS must provide the entire event payload. 3) Expected volume should be <1000 concurrent connections --8<-- \"docs/reference_links\"","title":"Studio Edge Gateway"},{"location":"advanced/type-resolvers-for-abstract-types/","text":"You must register type resolvers whenever you use interface types or union types in your schema. Interface types and union types are explained in the GraphQL documentation . As an example, the following schema defines a Movie interface type with two different concrete object type implementations. type Query { movies: [Movie] } interface Movie { title: String } type ScaryMovie implements Movie { title: String gory: Boolean scareFactor: Int } type ActionMovie implements Movie { title: String nrOfExplosions: Int } The following data fetcher is registered to return a list of movies. The data fetcher returns a combination Movie types. @DgsComponent public class MovieDataFetcher { @DgsData(parentType = \"Query\", field = \"movies\") public List<Movie> movies() { return Lists.newArrayList( new ActionMovie(\"Crouching Tiger\", 0), new ActionMovie(\"Black hawk down\", 10), new ScaryMovie(\"American Horror Story\", true, 10), new ScaryMovie(\"Love Death + Robots\", false, 4) ); } } The GraphQL runtime needs to know that a Java instance of ActionMovie represents the ActionMovie GraphQL type. This mapping is the responsibility of a TypeResolver . !!!info If your Java type names and GraphQL type names are the same, the DGS framework creates a TypeResolver automatically. No code needs to be added! Registering a Type Resolver If the name of your Java type and GraphQL type don't match, you need to provide a TypeResolver . A type resolver helps the framework map from concrete Java types to the correct object type in the schema. Use the @DgsTypeResolver annotation to register a type resolver. The annotation has a name property; set this to the name of the interface type or union type in the [GraphQL] schema. The resolver takes an object of the Java interface type, and returns a String which is the concrete object type of the instance as defined in the schema. The following is a type resolver for the Movie interface type introduced above: @DgsTypeResolver(name = \"Movie\") public String resolveMovie(Movie movie) { if(movie instanceof ScaryMovie) { return \"ScaryMovie\"; } else if(movie instanceof ActionMovie) { return \"ActionMovie\"; } else { throw new RuntimeException(\"Invalid type: \" + movie.getClass().getName() + \" found in MovieTypeResolver\"); } } You can add the @DgsTypeResolver annotation to any @DgsComponent class. This means you can either keep the type resolver in the same class as the data fetcher responsible for returning the data for this type, or you can create a separate class for it. Example Code The complete example used above can be found in MovieDataFetcher.java from the graphql-dgs-example-shared project . --8<-- \"docs/reference_links\"","title":"Type Resolvers for Abstract Types"},{"location":"advanced/type-resolvers-for-abstract-types/#registering-a-type-resolver","text":"If the name of your Java type and GraphQL type don't match, you need to provide a TypeResolver . A type resolver helps the framework map from concrete Java types to the correct object type in the schema. Use the @DgsTypeResolver annotation to register a type resolver. The annotation has a name property; set this to the name of the interface type or union type in the [GraphQL] schema. The resolver takes an object of the Java interface type, and returns a String which is the concrete object type of the instance as defined in the schema. The following is a type resolver for the Movie interface type introduced above: @DgsTypeResolver(name = \"Movie\") public String resolveMovie(Movie movie) { if(movie instanceof ScaryMovie) { return \"ScaryMovie\"; } else if(movie instanceof ActionMovie) { return \"ActionMovie\"; } else { throw new RuntimeException(\"Invalid type: \" + movie.getClass().getName() + \" found in MovieTypeResolver\"); } } You can add the @DgsTypeResolver annotation to any @DgsComponent class. This means you can either keep the type resolver in the same class as the data fetcher responsible for returning the data for this type, or you can create a separate class for it.","title":"Registering a Type Resolver"},{"location":"advanced/type-resolvers-for-abstract-types/#example-code","text":"The complete example used above can be found in MovieDataFetcher.java from the graphql-dgs-example-shared project . --8<-- \"docs/reference_links\"","title":"Example Code"}]}